{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QeQLP_jNVz7j",
        "outputId": "970b779a-bcc3-4475-fa4f-2530ae382663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.27-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.50)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.61-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.16)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.11.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.22.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting langchain-core<0.4,>=0.1 (from langgraph)\n",
            "  Downloading langchain_core-0.3.51-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.13.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langgraph-0.3.27-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading groq-0.22.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.51-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.3/423.3 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
            "Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.61-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: xxhash, python-dotenv, ormsgpack, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, langgraph-sdk, groq, dataclasses-json, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain_groq, langgraph-prebuilt, langchain, langgraph, langchain_community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.50\n",
            "    Uninstalling langchain-core-0.3.50:\n",
            "      Successfully uninstalled langchain-core-0.3.50\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.7\n",
            "    Uninstalling langchain-text-splitters-0.3.7:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.7\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.22\n",
            "    Uninstalling langchain-0.3.22:\n",
            "      Successfully uninstalled langchain-0.3.22\n",
            "Successfully installed dataclasses-json-0.6.7 groq-0.22.0 httpx-sse-0.4.0 langchain-0.3.23 langchain-core-0.3.51 langchain-text-splitters-0.3.8 langchain_community-0.3.21 langchain_groq-0.3.2 langgraph-0.3.27 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.61 marshmallow-3.26.1 mypy-extensions-1.0.0 ormsgpack-1.9.1 pydantic-settings-2.8.1 python-dotenv-1.1.0 typing-inspect-0.9.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph langsmith langchain langchain_groq langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('groq_api_key')"
      ],
      "metadata": {
        "id": "A9Qhe32kV21r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, TypedDict, Annotated, Sequence, Any\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "import operator\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "import json\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "import re\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing_extensions import Literal\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZKwhRMMvV23_",
        "outputId": "df09a7d2-8670-4b2f-c416-9fad38c74347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphState(TypedDict):  # Class GraphState\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    current_agent: str\n",
        "    current_llm_model: str\n",
        "    context_summary: str\n",
        "\n",
        "state = StateGraph(GraphState)"
      ],
      "metadata": {
        "id": "Nx0uzJCFeIJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_generator = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "system = \"\"\"You are a prompt generator for text to image model. Your task is to generate 50 diffrent prompts for text to image model to generate diverse images given to you\n",
        "the scenarios. example if I give you scenario as 'people in distress in the context to disaster' you give me different prompts for image generator model to\n",
        "generate a complete dataset of images of people in distress covering diverse scenarios. \"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt_generator_agent = prompt | prompt_generator\n",
        "\n",
        "def prompt_generator_handler(state: GraphState):\n",
        "    \"\"\" The marketing RAG powered Agent to resolve user queries\"\"\"\n",
        "\n",
        "    question = state[\"messages\"][-1].content\n",
        "\n",
        "    inbuilt_query = f\"\"\" You are a prompt generator for a text-to-image model. Given the user scenario and access to the full conversation history,\n",
        "your task is to generate 50 diverse and creative image generation prompts based on the scenario provided by the user.\n",
        "Make sure the prompts cover a wide range of sub-scenarios, perspectives, environments, and subjects to create a comprehensive dataset of images.\n",
        "what i need as the output exactly is: return the python list of 50 prompt strings. agent_output.content should be a string like: '[\"prompt1\", \"prompt2\", ...]'\n",
        "\n",
        "User scenario:\n",
        "{question}\n",
        "\n",
        "    \"\"\"\n",
        "    response = prompt_generator_agent.invoke(inbuilt_query)\n",
        "\n",
        "    # Add the question-response pair to conversation_history\n",
        "    conversation_id = len(state[\"conversation_history\"]) + 1\n",
        "    state[\"conversation_history\"][conversation_id] = {\n",
        "        \"question\": question,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    state[\"llm_model\"] = \"Gemma2-9b-It\"\n",
        "\n",
        "    # Also add the AI response to the messages list\n",
        "    state[\"messages\"].append(AIMessage(content=response.content))\n",
        "\n",
        "    # Update the current_agent within the state dictionary\n",
        "    state['current_agent'] = \"prompt_generator_agent\"  # Or the appropriate agent name\n",
        "\n",
        "    # Return the updated state\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "CdiqxrarV26F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_generator = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "system = \"\"\"You are a prompt generator for text to image model. Your task is to generate 50 diffrent prompts for text to image model to generate diverse images given to you\n",
        "the scenarios. example if I give you scenario as 'people in distress in the context to disaster' you give me different prompts for image generator model to\n",
        "generate a complete dataset of images of people in distress covering diverse scenarios. \"\"\"\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chat_agent = chat_prompt | chat_generator\n",
        "\n",
        "def chat_handler(state: GraphState):\n",
        "    \"\"\" The marketing RAG powered Agent to resolve user queries\"\"\"\n",
        "\n",
        "    question = state[\"messages\"][-1].content\n",
        "\n",
        "    inbuilt_query = f\"\"\" You are a prompt generator for a text-to-image model. Given the user scenario and access to the full conversation history,\n",
        "your task is to generate 50 diverse and creative image generation prompts based on the scenario provided by the user.\n",
        "Make sure the prompts cover a wide range of sub-scenarios, perspectives, environments, and subjects to create a comprehensive dataset of images.\n",
        "\n",
        "User scenario:\n",
        "{question}\n",
        "    \"\"\"\n",
        "    response = prompt_generator_agent.invoke(inbuilt_query)\n",
        "\n",
        "    # Add the question-response pair to conversation_history\n",
        "    conversation_id = len(state[\"conversation_history\"]) + 1\n",
        "    state[\"conversation_history\"][conversation_id] = {\n",
        "        \"question\": question,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    state[\"llm_model\"] = \"Gemma2-9b-It\"\n",
        "\n",
        "    # Also add the AI response to the messages list\n",
        "    state[\"messages\"].append(AIMessage(content=response.content))\n",
        "\n",
        "    # Update the current_agent within the state dictionary\n",
        "    state['current_agent'] = \"chat_agent\"  # Or the appropriate agent name\n",
        "\n",
        "    # Return the updated state\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "Silkk4JBG0bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Initialize the state\n",
        "    initial_state = {\n",
        "        \"messages\": [],\n",
        "        \"current_agent\": \"prompt_generator_agent\",  # Set initial agent\n",
        "        \"current_llm_model\": \"deepseek-r1-distill-qwen-32b\",\n",
        "        \"context_summary\": \"\",\n",
        "        \"conversation_history\": {} # Initialize conversation history\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"Enter a scenario (or type 'exit' to quit): \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Create a HumanMessage from the input\n",
        "        initial_state[\"messages\"].append(HumanMessage(content=user_input))\n",
        "\n",
        "        # Invoke the prompt_generator_handler\n",
        "        updated_state = prompt_generator_handler(initial_state)\n",
        "\n",
        "        # Print or process the updated state's AI response\n",
        "        print(updated_state[\"messages\"][-1].content)\n",
        "\n",
        "        # Update the initial state with the new state for the next iteration\n",
        "        initial_state = updated_state\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RS302m6eVGU",
        "outputId": "8d0da30f-c6b9-419a-acfe-34de56cea3f2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a scenario (or type 'exit' to quit): Birds\n",
            "```python\n",
            "agent_output = [\n",
            "    \"A majestic bald eagle soaring above snow-capped mountains at sunset\",\n",
            "    \"A hummingbird hovering in mid-air, sipping nectar from a vibrant flower\",\n",
            "    \"A flock of colorful parrots squawking in a lush rainforest canopy\",\n",
            "    \"A lone penguin waddling across a frozen Antarctic landscape\",\n",
            "    \"A pair of lovebirds perched on a tree branch, cuddling each other\",\n",
            "    \"A tiny wren singing a beautiful melody in a dew-kissed garden\",\n",
            "    \"A peacock spreading its iridescent tail feathers in a display of grandeur\",\n",
            "    \"A group of owls perched on branches, their glowing eyes observing the night\",\n",
            "    \"A majestic owl with its wings spread wide, silhouetted against a full moon\",\n",
            "    \"A cardinal perched on a snow-covered branch, its bright red plumage contrasting with the white\",\n",
            "    \"A flock of starlings flying in a mesmerizing V-formation across a clear blue sky\",\n",
            "    \"A blue jay perched on a fence post, its blue and white feathers shimmering in the sunlight\",\n",
            "    \"A woodpecker hammering away at a tree trunk, its red crest bobbing\",\n",
            "    \"A hawk soaring high above the ground, its keen eyes searching for prey\",\n",
            "    \"A family of ducks swimming in a calm pond, their ducklings trailing behind\",\n",
            "    \"A swan gliding gracefully across a lake, its white feathers reflecting the sunlight\",\n",
            "    \"A goose honking loudly, flying in formation with its flock\",\n",
            "    \"A vulture circling high above a vast desert landscape\",\n",
            "    \"A robin building a nest in a tree branch\",\n",
            "    \"A sparrow hopping on the ground, searching for seeds\",\n",
            "    \"A hummingbird feeding on a flower, its wings beating rapidly\",\n",
            "    \"A parrot mimicking the sounds of a human voice\",\n",
            "    \"A crow perched on a rooftop, its dark feathers blending with the shadows\",\n",
            "    \"A pigeon landing on a statue in a city park\",\n",
            "    \"A flock of geese flying south for the winter\",\n",
            "    \"A snowy owl perched on a snowdrift, its white plumage camouflaged against the snowy backdrop\",\n",
            "    \"A bluebird singing in a tree\",\n",
            "    \"A butterfly perched on a bird's beak\",\n",
            "    \"A bird's nest filled with eggs\",\n",
            "    \"A bird flying through a thunderstorm\",\n",
            "    \"A bird's eye view of a forest\",\n",
            "    \"A bird's feather close-up\",\n",
            "    \"A bird skeleton\",\n",
            "    \"An abstract representation of a bird in flight\",\n",
            "    \"A bird made entirely of flowers\",\n",
            "    \"A surreal landscape with giant birds flying overhead\",\n",
            "    \"A dystopian city where birds are extinct\",\n",
            "    \"A dreamlike scene with birds transforming into other creatures\",\n",
            "    \"A bird symbolizing freedom and hope\",\n",
            "    \"A bird representing loneliness and isolation\",\n",
            "    \"A bird representing wisdom and knowledge\",\n",
            "    \"A bird representing beauty and grace\",\n",
            "    \"A bird representing danger and fear\",\n",
            "    \"A bird representing the cycle of life and death\",\n",
            "    \"A bird representing the connection between humans and nature\"\n",
            "]\n",
            "\n",
            "```\n",
            "\n",
            "Enter a scenario (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import requests\n",
        "import os\n",
        "import ast\n",
        "from time import sleep\n",
        "\n",
        "# Set OpenAI API key from Colab's env\n",
        "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "# Directory to save images\n",
        "output_dir = \"/content/generated_images\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "initial_state = {\n",
        "        \"messages\": [],\n",
        "        \"current_agent\": \"prompt_generator_agent\",  # Set initial agent\n",
        "        \"current_llm_model\": \"deepseek-r1-distill-qwen-32b\",\n",
        "        \"context_summary\": \"\",\n",
        "        \"conversation_history\": {} # Initialize conversation history\n",
        "    }\n",
        "\n",
        "# === ðŸ” STEP 1: Invoke your prompt generator agent ===\n",
        "scenario = input(\"Enter a scenario (or type 'exit' to quit): \")\n",
        "# agent_output = prompt_generator_agent.invoke(scenario)\n",
        "\n",
        "initial_state[\"messages\"].append(HumanMessage(content=scenario))\n",
        "\n",
        "# Invoke the prompt_generator_handler\n",
        "updated_state = prompt_generator_handler(initial_state)\n",
        "\n",
        "# Print or process the updated state's AI response\n",
        "agent_output = updated_state[\"messages\"][-1].content\n",
        "print(updated_state[\"messages\"][-1].content)\n",
        "\n",
        "# Update the initial state with the new state for the next iteration\n",
        "initial_state = updated_state\n",
        "\n",
        "\n",
        "# === ðŸ§  STEP 2: Parse output string to Python list ===\n",
        "# agent_output.content should be a string like: '[\"prompt1\", \"prompt2\", ...]'\n",
        "\n",
        "\n",
        "try:\n",
        "    # Attempt to parse as JSON first\n",
        "    prompt_list = json.loads(agent_output)\n",
        "\n",
        "    # If JSON parsing fails, fallback to ast.literal_eval\n",
        "except json.JSONDecodeError:\n",
        "    try:\n",
        "        prompt_list = ast.literal_eval(agent_output)\n",
        "    except (SyntaxError, ValueError) as e:\n",
        "        # If both JSON and literal_eval fail, extract prompts using regex\n",
        "        prompt_pattern = r'\"(.*?)\"'  # Matches text within double quotes\n",
        "        prompt_list = re.findall(prompt_pattern, agent_output)\n",
        "        if not prompt_list:\n",
        "            raise ValueError(f\"Failed to extract prompts using regex: {e}\")\n",
        "\n",
        "    if not isinstance(prompt_list, list):\n",
        "        raise ValueError(\"Parsed output is not a list.\")\n",
        "\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Failed to parse prompts from agent: {e}\")\n",
        "\n",
        "\n",
        "print(f\"âœ… Got {len(prompt_list)} prompts from agent!\")\n",
        "\n",
        "# === ðŸŽ¨ STEP 3: Generate and save images ===\n",
        "client = openai.OpenAI(\n",
        "    api_key= userdata.get(\"OPENAI_API_KEY\")\n",
        ")\n",
        "for idx, prompt in enumerate(prompt_list):\n",
        "    try:\n",
        "        print(f\"[{idx+1}/{len(prompt_list)}] Generating image for prompt: {prompt}\")\n",
        "\n",
        "        response = client.images.generate(\n",
        "            model=\"dall-e-2\",  # or \"dall-e-2\"\n",
        "            prompt=prompt,\n",
        "            size=\"256x256\",\n",
        "            quality=\"standard\",\n",
        "            n=1\n",
        "        )\n",
        "\n",
        "        image_url = response[\"data\"][0][\"url\"]\n",
        "        image_data = requests.get(image_url).content\n",
        "        file_path = os.path.join(output_dir, f\"image_{idx+1:02}.png\")\n",
        "\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(image_data)\n",
        "\n",
        "        print(f\"âœ… Saved: {file_path}\")\n",
        "        sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error with prompt {idx+1}: {e}\")\n"
      ],
      "metadata": {
        "id": "pVNemTlLV28W",
        "outputId": "da0201da-c270-473e-8039-6af3bf6e9cb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a scenario (or type 'exit' to quit): trees\n",
            "```python\n",
            "[\"A towering redwood sequoia bathed in dappled sunlight, towering over a lush forest floor.\", \n",
            "\"A lone, gnarled oak tree standing sentinel on a windswept hilltop, branches reaching towards the stormy sky.\",\n",
            "\"A grove of ancient olive trees, their silvery leaves shimmering in the Mediterranean sun.\",\n",
            "\"A vibrant Japanese maple, ablaze with autumn colors, leaves cascading down like fiery rain.\",\n",
            "\"Twisted and ancient cypress trees lining a foggy Italian path, creating an eerie and mysterious atmosphere.\",\n",
            "\"A young sapling, its delicate leaves unfurling in the spring breeze, surrounded by wildflowers.\",\n",
            "\"A dense forest of towering pine trees, their needles carpeting the ground in a thick layer of green.\",\n",
            "\"A panoramic view of a sprawling forest, sunlight filtering through the leaves, creating a mosaic of light and shadow.\",\n",
            "\"A close-up shot of a tree bark, showcasing its intricate patterns and textures.\",\n",
            "\"A cluster of birch trees, their white bark glowing in the moonlight, casting long shadows on the forest floor.\",\n",
            "\"A deciduous forest in full bloom, flowers carpeting the ground beneath the canopy of leaves.\",\n",
            "\"An abstract representation of a tree, using lines and shapes to capture its essence.\",\n",
            "\"A tree silhouette against a fiery sunset, casting a long, dramatic shadow.\",\n",
            "\"A surreal image of a tree growing through a cityscape, its roots reaching towards the sky.\",\n",
            "\"A whimsical illustration of a treehouse nestled amongst the branches of a giant oak.\",\n",
            "\"A magical forest scene, with glowing mushrooms and bioluminescent trees.\",\n",
            "\"A winter wonderland, with snow-covered trees and frozen lakes.\",\n",
            "\"A dense jungle, with towering trees and exotic flowers.\",\n",
            "\"A desert oasis, with a lone palm tree providing shade and water.\",\n",
            "\"A futuristic cityscape, with towering trees incorporated into the architecture.\",\n",
            "\"A tree with glowing, bioluminescent leaves, illuminating the forest at night.\",\n",
            "\"A tree growing out of a cracked earth, symbolizing resilience and hope.\",\n",
            "\"A tree with roots reaching deep into the ocean, connecting to the depths of the earth.\",\n",
            "\"A tree with fruit that resembles human faces, creating a sense of surrealism.\",\n",
            "\"A tree with branches that transform into birds in flight.\",\n",
            "\"A tree with leaves that change color with the viewer's emotions.\",\n",
            "\"A tree with its branches forming the shape of a human figure.\",\n",
            "\"A tree with its roots forming a labyrinthine maze.\",\n",
            "\"A tree with its trunk covered in ancient hieroglyphics.\",\n",
            "\"A tree with its leaves forming a protective canopy over a village below.\",\n",
            "\"A tree with its branches stretching towards the stars, symbolizing aspirations and dreams.\",\n",
            "\"A tree sculpted from ice, shimmering in the sunlight.\",\n",
            "\"A tree made entirely of flowers, blooming in all colors.\",\n",
            "\"A tree with its roots forming a giant heart, symbolizing love and connection.\",\n",
            "\"A tree with its leaves whispering secrets to the wind.\",\n",
            "\"A tree with its branches reaching out to embrace the world.\",\n",
            "\"A tree with its trunk glowing with an ethereal light.\",\n",
            "\"A tree that transcends the boundaries of reality, existing in multiple dimensions.\",\n",
            "\"A tree that represents the cycle of life, death, and rebirth.\"] \n",
            "``` \n",
            "\n",
            "\n",
            "\n",
            "âœ… Got 39 prompts from agent!\n",
            "[1/39] Generating image for prompt: A towering redwood sequoia bathed in dappled sunlight, towering over a lush forest floor.\n",
            "âŒ Error with prompt 1: Error code: 400 - {'error': {'code': 'billing_hard_limit_reached', 'message': 'Billing hard limit has been reached', 'param': None, 'type': 'invalid_request_error'}}\n",
            "[2/39] Generating image for prompt: A lone, gnarled oak tree standing sentinel on a windswept hilltop, branches reaching towards the stormy sky.\n",
            "âŒ Error with prompt 2: Error code: 400 - {'error': {'code': 'billing_hard_limit_reached', 'message': 'Billing hard limit has been reached', 'param': None, 'type': 'invalid_request_error'}}\n",
            "[3/39] Generating image for prompt: A grove of ancient olive trees, their silvery leaves shimmering in the Mediterranean sun.\n",
            "âŒ Error with prompt 3: Error code: 400 - {'error': {'code': 'billing_hard_limit_reached', 'message': 'Billing hard limit has been reached', 'param': None, 'type': 'invalid_request_error'}}\n",
            "[4/39] Generating image for prompt: A vibrant Japanese maple, ablaze with autumn colors, leaves cascading down like fiery rain.\n",
            "âŒ Error with prompt 4: Error code: 400 - {'error': {'code': 'billing_hard_limit_reached', 'message': 'Billing hard limit has been reached', 'param': None, 'type': 'invalid_request_error'}}\n",
            "[5/39] Generating image for prompt: Twisted and ancient cypress trees lining a foggy Italian path, creating an eerie and mysterious atmosphere.\n",
            "âŒ Error with prompt 5: Error code: 400 - {'error': {'code': 'billing_hard_limit_reached', 'message': 'Billing hard limit has been reached', 'param': None, 'type': 'invalid_request_error'}}\n",
            "[6/39] Generating image for prompt: A young sapling, its delicate leaves unfurling in the spring breeze, surrounded by wildflowers.\n",
            "âŒ Error with prompt 6: Error code: 429 - {'error': {'code': 'rate_limit_exceeded', 'message': 'Rate limit exceeded for images per minute in organization org-t80GchCytxtJrChnmmxg6lXm. Limit: 5/1min. Current: 6/1min. Please visit https://platform.openai.com/docs/guides/rate-limits to learn how to increase your rate limit.', 'param': None, 'type': 'requests'}}\n",
            "[7/39] Generating image for prompt: A dense forest of towering pine trees, their needles carpeting the ground in a thick layer of green.\n",
            "âŒ Error with prompt 7: Error code: 429 - {'error': {'code': 'rate_limit_exceeded', 'message': 'Rate limit exceeded for images per minute in organization org-t80GchCytxtJrChnmmxg6lXm. Limit: 5/1min. Current: 6/1min. Please visit https://platform.openai.com/docs/guides/rate-limits to learn how to increase your rate limit.', 'param': None, 'type': 'requests'}}\n",
            "[8/39] Generating image for prompt: A panoramic view of a sprawling forest, sunlight filtering through the leaves, creating a mosaic of light and shadow.\n",
            "âŒ Error with prompt 8: Error code: 429 - {'error': {'code': 'rate_limit_exceeded', 'message': 'Rate limit exceeded for images per minute in organization org-t80GchCytxtJrChnmmxg6lXm. Limit: 5/1min. Current: 6/1min. Please visit https://platform.openai.com/docs/guides/rate-limits to learn how to increase your rate limit.', 'param': None, 'type': 'requests'}}\n",
            "[9/39] Generating image for prompt: A close-up shot of a tree bark, showcasing its intricate patterns and textures.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# thrown on 4xx and 5xx status code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/images/generations'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# thrown on 4xx and 5xx status code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/images/generations'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-d40bdfa72595>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[{idx+1}/{len(prompt_list)}] Generating image for prompt: {prompt}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         response = client.images.generate(\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dall-e-2\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# or \"dall-e-2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/images.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompt, model, n, quality, response_format, size, style, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \"\"\"\n\u001b[0;32m--> 264\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;34m\"/images/generations\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1009\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1058\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1009\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;31m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;31m# different thread if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         return self._request(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph langsmith langchain langchain_groq langchain_community langchain_openai FastAPI uvicorn"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unTgriWUuYMw",
        "outputId": "00cf828e-0596-4a82-a5fa-a3469a51a4c3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain_groq in /usr/local/lib/python3.11/dist-packages (0.3.2)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: FastAPI in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.51)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.24)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.61)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.16)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.11.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from langchain_groq) (0.22.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.8.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.70.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from FastAPI) (0.46.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from FastAPI) (4.13.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn\n",
            "Successfully installed uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Dict, List, Any, Optional, Literal\n",
        "from fastapi import FastAPI, HTTPException, Body, Depends\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_openai import OpenAI\n",
        "import openai\n",
        "from langgraph.graph import StateGraph, END\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from contextlib import asynccontextmanager"
      ],
      "metadata": {
        "id": "NcZdo9WeV2-u"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphState(TypedDict):\n",
        "    messages: List[Any]  # The messages passed between user and assistant\n",
        "    conversation_history: Dict  # A record of all conversation threads\n",
        "    current_agent: str  # Track which agent is currently active\n",
        "    prompt_list: List[str]  # Store generated prompts\n",
        "    sample_images: List[str]  # Store URLs of sample images\n",
        "    user_feedback: str  # Store user feedback on sample images\n",
        "    dataset_ready: bool  # Flag to indicate if user wants the full dataset\n",
        "    llm_model: str  # Track which LLM model is being used\n",
        "    session_id: str  # Unique identifier for the conversation session\n",
        "    scenario: str   # We'll store the scenarion and update it accordingly"
      ],
      "metadata": {
        "id": "IoS_BAAJV3B5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variables to store active sessions\n",
        "active_sessions = {}\n",
        "\n",
        "def initialize_agents(groq_api_key, openai_api_key):  # Add initial_scenario argument\n",
        "    \"\"\"Initialize all the agent components\"\"\"\n",
        "    conversation_agent = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "    prompt_generator = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "    image_client = openai.OpenAI(api_key=openai_api_key)\n",
        "\n",
        "    # Router agent\n",
        "    router_agent = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "\n",
        "    return conversation_agent, prompt_generator, image_client, router_agent\n",
        "\n",
        "def init_img_steeings(state, num, dimensions, initial_scenario):\n",
        "  state['scenario'] = initial_scenario\n",
        "  # also set dimansions and number of images\n"
      ],
      "metadata": {
        "id": "kJE2h6VxufXs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Helper Functions"
      ],
      "metadata": {
        "id": "XEdQRcYGznfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def extract_scenario(state):\n",
        "#     \"\"\"Extract the user's scenario from the conversation history\"\"\"\n",
        "#     relevant_messages = []\n",
        "\n",
        "#     for message in state[\"messages\"]:\n",
        "#         if isinstance(message, HumanMessage):\n",
        "#             relevant_messages.append(message.content)\n",
        "\n",
        "#     # Concatenate all user messages into a single scenario description\n",
        "#     if relevant_messages:\n",
        "#         combined_scenario = \" \".join(relevant_messages)\n",
        "#         # If too long, use only the most substantial messages\n",
        "#         if len(combined_scenario) > 500:\n",
        "#             # Find the longest message, which likely contains the main scenario\n",
        "#             longest_message = max(relevant_messages, key=len)\n",
        "#             return longest_message\n",
        "#         return combined_scenario\n",
        "\n",
        "#     return \"generic dataset of various objects and scenes\"\n",
        "\n",
        "\n",
        "def get_current_user_question(state):\n",
        "    \"\"\"Extract the most recent user question from the state\"\"\"\n",
        "    for message in reversed(state[\"messages\"]):\n",
        "        if isinstance(message, HumanMessage):\n",
        "            return message.content\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def parse_image_prompts(response_text):\n",
        "    \"\"\"Extract a list of prompts from the LLM response\"\"\"\n",
        "    try:\n",
        "        # Try to find a Python list in the text\n",
        "        match = re.search(r'\\[(.*?)\\]', response_text, re.DOTALL)\n",
        "        if match:\n",
        "            prompts_text = match.group(1)\n",
        "            # Safely evaluate the list\n",
        "            return eval(f\"[{prompts_text}]\")\n",
        "\n",
        "        # If no list is found, try to extract numbered items\n",
        "        lines = response_text.split('\\n')\n",
        "        prompts = []\n",
        "        for line in lines:\n",
        "            # Look for numbered lines like \"1. prompt\" or \"1) prompt\"\n",
        "            match = re.match(r'^\\s*\\d+[\\.\\)]\\s*(.*)', line)\n",
        "            if match:\n",
        "                prompts.append(match.group(1).strip())\n",
        "\n",
        "        if prompts:\n",
        "            return prompts\n",
        "\n",
        "        # If all else fails, split by newlines and filter\n",
        "        return [line.strip() for line in lines if line.strip() and len(line.strip()) > 10]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing prompts: {e}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "ZH4SpFOpufUd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def updated_scenario(state):\n",
        "  scenario = state['scenario']\n",
        "  scenario_update_llm = ChatGroq(groq_api_key=groq_api_key, model=\"llama-3.1-8b-instant\")\n",
        "\n",
        "  system = f\"\"\" You have a scenario which will be used to generate diverse prompts each will be used for\n",
        "  for image generation. What your task is I will provide you initial scenario and you have to update it\n",
        "  accordingly to the user instructions. You will have both user instructions and the scenario, return the updated scenario please.\n",
        "  Scenario: {scenario}\n",
        "  Instruction from User: {state['messages'][-1]}\n",
        "  \"\"\"\n",
        "  # Update the prompt based on the given user instruction\n",
        "  scenario_update_prompt = ChatPromptTemplate.from_messages(\n",
        "      [\n",
        "          (\"system\", system),\n",
        "          (\"human\", \"{question}\"),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  scenario_update_agent = scenario_update_prompt | scenario_update_llm\n",
        "  response = scenario_update_agent.invoke(state['messages'][-1])\n",
        "\n",
        "  #state['scenario'] = response.content\n",
        "  return response.content\n"
      ],
      "metadata": {
        "id": "vDECCH6ZufSU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Router"
      ],
      "metadata": {
        "id": "gro1TnNG6oBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_router(router_agent):\n",
        "    \"\"\"Create an LLM router to determine which agent should handle the request\"\"\"\n",
        "\n",
        "    # Use JsonOutputParser to format LLM output\n",
        "    parser = JsonOutputParser(pydantic_object=RouterOutput)\n",
        "\n",
        "    system = \"\"\"You are an expert routing agent for an image dataset creation system.\n",
        "    Your job is to analyze the user's message and determine which of the following agents should handle it:\n",
        "\n",
        "    1. conversation_agent: For general conversation, questions about the system, or when the user is still describing their needs.\n",
        "\n",
        "    2. sample_image_agent: When the user is explicitly or implicitly requesting sample images, wants to see examples,\n",
        "       or is describing specific image requirements that would benefit from visual confirmation before proceeding.\n",
        "\n",
        "    3. dataset_generator_agent: When the user is ready to generate the full dataset of images, expressing satisfaction with samples,\n",
        "       or directly asking to proceed with dataset creation.\n",
        "\n",
        "    Analyze the content, context, and intent of the user's message, then select the most appropriate agent.\n",
        "\n",
        "    Output your decision as a JSON object with the following fields:\n",
        "    - agent: The name of the agent (one of the three above)\n",
        "    - confidence: Your confidence level in this decision (0.0 to 1.0)\n",
        "    - reasoning: A brief explanation of why you chose this agent\n",
        "    \"\"\"\n",
        "\n",
        "    router_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"User message: {question}\\n\\nConversation history summary: {history_summary}\")\n",
        "    ])\n",
        "\n",
        "    router_chain = router_prompt | router_agent | parser\n",
        "\n",
        "    return router_chain\n"
      ],
      "metadata": {
        "id": "rWeDiBH24AQ2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a summary of conversation history\n",
        "def summarize_conversation(state):\n",
        "    \"\"\"Create a brief summary of the conversation history for context\"\"\"\n",
        "    history = []\n",
        "\n",
        "    # Extract last few turns of conversation\n",
        "    messages = state[\"messages\"][-10:]  # Limit to last 10 messages\n",
        "\n",
        "    for msg in messages:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            history.append(f\"User: {msg.content}\")\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            # Truncate very long assistant responses\n",
        "            content = msg.content\n",
        "            if len(content) > 200:\n",
        "                content = content[:197] + \"...\"\n",
        "            history.append(f\"Assistant: {content}\")\n",
        "\n",
        "    return \"\\n\".join(history)\n",
        "\n",
        "\n",
        "# Routing function\n",
        "def route_question(state: GraphState, router_chain):\n",
        "    \"\"\"Route the user question to the appropriate agent\"\"\"\n",
        "\n",
        "    question = get_current_user_question(state)\n",
        "    history_summary = summarize_conversation(state)\n",
        "\n",
        "    # Make routing decision\n",
        "    try:\n",
        "        route_decision = router_chain.invoke({\n",
        "            \"question\": question,\n",
        "            \"history_summary\": history_summary\n",
        "        })\n",
        "\n",
        "        print(f\"Router Decision: {route_decision}\")\n",
        "\n",
        "        # Update state to track which agent was selected\n",
        "        state[\"current_agent\"] = route_decision.agent\n",
        "\n",
        "        # Set dataset_ready flag if we're routing to dataset generator\n",
        "        if route_decision.agent == \"dataset_generator_agent\":\n",
        "            state[\"dataset_ready\"] = True\n",
        "\n",
        "        # Map agent names to graph nodes\n",
        "        agent_to_node = {\n",
        "            \"conversation_agent\": \"conversation\",\n",
        "            \"sample_image_agent\": \"generate_sample_images\",\n",
        "            \"dataset_generator_agent\": \"generate_prompts\"\n",
        "        }\n",
        "\n",
        "        return agent_to_node[route_decision.agent]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Routing error: {e}\")\n",
        "        # Default to conversation as fallback\n",
        "        return \"conversation\""
      ],
      "metadata": {
        "id": "AuT3HvqIufN2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent Handlers"
      ],
      "metadata": {
        "id": "pjHFigtt8syV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conversation_handler(state: GraphState):\n",
        "    \"\"\"The primary conversation agent to interact with the user\"\"\"\n",
        "\n",
        "    user_message = state[\"messages\"][-1].content\n",
        "\n",
        "    system_message = \"\"\"You are an AI assistant specializing in helping users create image datasets.\n",
        "    Your role is to understand what kind of image dataset the user wants to create.\n",
        "\n",
        "    Be conversational and helpful, focusing on understanding the user's requirements clearly.\n",
        "    If the user seems to be describing a specific type of images, suggest they might want to see samples.\n",
        "    If they've seen samples and like them, you can suggest generating the full dataset.\n",
        "\n",
        "    Keep your responses concise and focused on helping create their image dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_message),\n",
        "        (\"human\", \"{user_input}\")\n",
        "    ])\n",
        "\n",
        "    conversation_agent = state[\"conversation_agent\"]\n",
        "    chain = prompt | conversation_agent\n",
        "    response = chain.invoke({\"user_input\": user_message})\n",
        "\n",
        "    # Add to conversation history\n",
        "    conversation_id = len(state[\"conversation_history\"]) + 1\n",
        "    state[\"conversation_history\"][conversation_id] = {\n",
        "        \"question\": user_message,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    state[\"messages\"].append(AIMessage(content=response.content))\n",
        "    state[\"current_agent\"] = \"conversation_agent\"\n",
        "    state['scenario'] = updated_scenario(state)\n",
        "\n",
        "    return \"router\"\n",
        "\n",
        "\n",
        "def sample_image_handler(state: GraphState):\n",
        "    \"\"\"Generate sample images for user review\"\"\"\n",
        "\n",
        "    # Extract user scenario from conversation\n",
        "    user_scenario = state['scenario']\n",
        "\n",
        "    # Generate a few sample prompts\n",
        "    system = \"\"\"You are a prompt generator for text-to-image models.\n",
        "    Generate 3 diverse and detailed prompts for sample images based on the user scenario.\n",
        "    Each prompt should be descriptive and optimized for image generation.\n",
        "    Format your output as a Python list of exactly 3 strings: [\"prompt1\", \"prompt2\", \"prompt3\"]\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system),\n",
        "        (\"human\", f\"Generate 3 sample image prompts for scenario: {user_scenario}\")\n",
        "    ])\n",
        "\n",
        "    prompt_generator = state[\"prompt_generator\"]\n",
        "    chain = prompt | prompt_generator\n",
        "    response = chain.invoke({})\n",
        "\n",
        "    # Extract the prompt list\n",
        "    sample_prompts = parse_image_prompts(response.content)\n",
        "\n",
        "    # Ensure we have exactly 3 prompts\n",
        "    if len(sample_prompts) < 3:\n",
        "        # Fill in missing prompts\n",
        "        while len(sample_prompts) < 3:\n",
        "            sample_prompts.append(f\"{user_scenario} variation {len(sample_prompts)+1}\")\n",
        "    elif len(sample_prompts) > 3:\n",
        "        # Trim to just 3 prompts\n",
        "        sample_prompts = sample_prompts[:3]\n",
        "\n",
        "    # Generate sample images\n",
        "    sample_image_urls = []\n",
        "    for prompt in sample_prompts:\n",
        "        try:\n",
        "            response = state[\"image_client\"].images.generate(\n",
        "                model=\"dall-e-2\",\n",
        "                prompt=prompt,\n",
        "                size=\"256x256\",\n",
        "                quality=\"standard\",\n",
        "                n=1\n",
        "            )\n",
        "            sample_image_urls.append(response.data[0].url)\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating image: {str(e)}\")\n",
        "            sample_image_urls.append(None)\n",
        "\n",
        "    # Filter out any None values\n",
        "    sample_image_urls = [url for url in sample_image_urls if url is not None]\n",
        "\n",
        "    state[\"sample_images\"] = sample_image_urls\n",
        "    state[\"prompt_list\"] = sample_prompts\n",
        "\n",
        "    # Create response message with sample images\n",
        "    response_text = \"Here are some sample images based on your requirements:\\n\\n\"\n",
        "    for i, (prompt, url) in enumerate(zip(sample_prompts, sample_image_urls)):\n",
        "        if url:\n",
        "            response_text += f\"Sample {i+1}:\\nPrompt: {prompt}\\n\\n\"\n",
        "\n",
        "    response_text += \"How do these look? Would you like to make any adjustments before generating the full dataset of 50 images?\"\n",
        "\n",
        "    state[\"messages\"].append(AIMessage(content=response_text))\n",
        "    state[\"current_agent\"] = \"sample_image_agent\"\n",
        "\n",
        "    return \"router\"\n",
        "\n"
      ],
      "metadata": {
        "id": "JqkWCfGD8u9x"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_generator_handler(state: GraphState):\n",
        "    \"\"\"Generate the full set of prompts based on user requirements\"\"\"\n",
        "\n",
        "    # Extract user scenario from conversation\n",
        "    user_scenario = extract_scenario(state)\n",
        "    user_feedback = state.get(\"user_feedback\", \"\")\n",
        "\n",
        "    # Include user feedback if available\n",
        "    scenario_with_feedback = user_scenario\n",
        "    if user_feedback:\n",
        "        scenario_with_feedback = f\"{user_scenario}. User feedback on samples: {user_feedback}\"\n",
        "\n",
        "    system = \"\"\"You are a prompt generator for text to image models. Your task is to generate 50 diverse prompts\n",
        "    for text-to-image models to create a comprehensive dataset given the scenario.\n",
        "    Each prompt should be descriptive and optimized for image generation.\n",
        "    Make sure to cover different angles, perspectives, environments, and contexts.\n",
        "    Format your output as a Python list of strings: [\"prompt1\", \"prompt2\", ...]\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system),\n",
        "        (\"human\", f\"Generate 50 diverse image prompts for scenario: {scenario_with_feedback}\")\n",
        "    ])\n",
        "\n",
        "    prompt_generator = state[\"prompt_generator\"]\n",
        "    chain = prompt | prompt_generator\n",
        "    response = chain.invoke({})\n",
        "\n",
        "    # Extract the prompt list\n",
        "    prompt_list = parse_image_prompts(response.content)\n",
        "\n",
        "    # Ensure we have at least 50 prompts\n",
        "    if len(prompt_list) < 50:\n",
        "        # Fill in missing prompts\n",
        "        while len(prompt_list) < 50:\n",
        "            prompt_list.append(f\"{user_scenario} variation {len(prompt_list)+1}\")\n",
        "\n",
        "    state[\"prompt_list\"] = prompt_list[:50]  # Limit to exactly 50 prompts\n",
        "    state[\"dataset_ready\"] = True\n",
        "\n",
        "    # Prepare response message\n",
        "    response_text = f\"I've generated 50 diverse prompts for your dataset based on your requirements.\\n\\n\"\n",
        "    response_text += \"Here are a few examples:\\n\"\n",
        "    for i, prompt in enumerate(prompt_list[:5]):\n",
        "        response_text += f\"{i+1}. {prompt}\\n\"\n",
        "\n",
        "    response_text += f\"\\n...and {len(prompt_list)-5} more prompts.\\n\\n\"\n",
        "    response_text += \"Would you like me to proceed with generating all 50 images for your dataset now?\"\n",
        "\n",
        "    state[\"messages\"].append(AIMessage(content=response_text))\n",
        "    state[\"current_agent\"] = \"dataset_generator_agent\"\n",
        "\n",
        "    return \"router\"\n",
        "\n",
        "\n",
        "def dataset_generator_handler(state: GraphState):\n",
        "    \"\"\"Generate the full dataset of images\"\"\"\n",
        "\n",
        "    prompt_list = state[\"prompt_list\"][:50]  # Ensure we use max 50 prompts\n",
        "    image_urls = []\n",
        "\n",
        "    # Create a progress update message\n",
        "    progress_message = \"Starting to generate your dataset with 50 images. This might take some time...\\n\"\n",
        "    state[\"messages\"].append(AIMessage(content=progress_message))\n",
        "\n",
        "    # Generate images for each prompt - for real implementation\n",
        "    for idx, prompt in enumerate(prompt_list):\n",
        "        try:\n",
        "            print(f\"[{idx+1}/{len(prompt_list)}] Generating image for prompt: {prompt}\")\n",
        "\n",
        "            response = state[\"image_client\"].images.generate(\n",
        "                model=\"dall-e-2\",\n",
        "                prompt=prompt,\n",
        "                size=\"256x256\",\n",
        "                quality=\"standard\",\n",
        "                n=1\n",
        "            )\n",
        "\n",
        "            image_url = response.data[0].url\n",
        "            image_urls.append(image_url)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error generating image {idx+1}: {str(e)}\"\n",
        "            print(error_msg)\n",
        "            image_urls.append(None)\n",
        "\n",
        "    # Filter out None values\n",
        "    image_urls = [url for url in image_urls if url is not None]\n",
        "\n",
        "    # Final response with dataset information\n",
        "    final_message = f\"âœ… Dataset generation complete! Generated {len(image_urls)}/50 images.\\n\\n\"\n",
        "\n",
        "    # Include some sample URLs\n",
        "    if image_urls:\n",
        "        final_message += \"Here are a few sample images from your dataset:\\n\"\n",
        "        for i, url in enumerate(image_urls[:5]):\n",
        "            if url:\n",
        "                final_message += f\"Image {i+1}: {url}\\n\"\n",
        "\n",
        "    final_message += \"\\nYou can download these images to assemble your complete dataset. Would you like me to help with anything else?\"\n",
        "\n",
        "    state[\"messages\"].append(AIMessage(content=final_message))\n",
        "    state[\"current_agent\"] = \"dataset_generator_agent\"\n",
        "\n",
        "    return \"router\"\n"
      ],
      "metadata": {
        "id": "lrYZ4jDs8vpq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Router Handler"
      ],
      "metadata": {
        "id": "bIiku9D2Dy9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Router handler function\n",
        "def router_handler(state: GraphState):\n",
        "    \"\"\"Route the conversation to the appropriate handler based on LLM decision\"\"\"\n",
        "    router_chain = state[\"router_chain\"]\n",
        "    return route_question(state, router_chain)"
      ],
      "metadata": {
        "id": "R-MeE14JDvRI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langgraph application"
      ],
      "metadata": {
        "id": "T2tRKSxaDrkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the LangGraph application\n",
        "def create_image_dataset_graph(agents):\n",
        "    conversation_agent, prompt_generator, image_client, router_agent = agents\n",
        "\n",
        "    # Create the router chain\n",
        "    router_chain = create_router(router_agent)\n",
        "\n",
        "    # Create the graph\n",
        "    workflow = StateGraph(GraphState)\n",
        "\n",
        "    # Add nodes\n",
        "    workflow.add_node(\"conversation\", conversation_handler)\n",
        "    workflow.add_node(\"generate_sample_images\", sample_image_handler)\n",
        "    workflow.add_node(\"generate_prompts\", prompt_generator_handler)\n",
        "    workflow.add_node(\"generate_dataset\", dataset_generator_handler)\n",
        "    workflow.add_node(\"router\", router_handler)\n",
        "\n",
        "    # Add edges from router to specific agents\n",
        "    workflow.add_edge(\"router\", \"conversation\")\n",
        "    workflow.add_edge(\"router\", \"generate_sample_images\")\n",
        "    workflow.add_edge(\"router\", \"generate_prompts\")\n",
        "\n",
        "    # Add edges from agents back to router\n",
        "    workflow.add_edge(\"conversation\", \"router\")\n",
        "    workflow.add_edge(\"generate_sample_images\", \"router\")\n",
        "    workflow.add_edge(\"generate_prompts\", \"router\")\n",
        "\n",
        "    # Add conditional edge from router to dataset generator\n",
        "    workflow.add_conditional_edges(\n",
        "        \"router\",\n",
        "        lambda state: state.get(\"dataset_ready\", False),\n",
        "        {\n",
        "            True: \"generate_dataset\",\n",
        "            False: \"router\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Dataset generator goes back to router when done\n",
        "    workflow.add_edge(\"generate_dataset\", \"router\")\n",
        "\n",
        "    # Compile the graph\n",
        "    app = workflow.compile()\n",
        "\n",
        "    return app, router_chain\n"
      ],
      "metadata": {
        "id": "P4VdqnnjDq3w"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Session Management functions"
      ],
      "metadata": {
        "id": "mquGRvyLDl0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_new_session(session_id, agents, router_chain):\n",
        "    \"\"\"Create a new conversation session\"\"\"\n",
        "    conversation_agent, prompt_generator, image_client, router_agent = agents\n",
        "\n",
        "    # Initialize the state\n",
        "    initial_state = {\n",
        "        \"messages\": [AIMessage(content=\"Hello! I'm your image dataset creation assistant. What kind of image dataset would you like to create today?\")],\n",
        "        \"conversation_history\": {},\n",
        "        \"current_agent\": \"conversation_agent\",\n",
        "        \"prompt_list\": [],\n",
        "        \"sample_images\": [],\n",
        "        \"user_feedback\": \"\",\n",
        "        \"dataset_ready\": False,\n",
        "        \"llm_model\": \"Gemma2-9b-It\",\n",
        "        \"conversation_agent\": conversation_agent,\n",
        "        \"prompt_generator\": prompt_generator,\n",
        "        \"image_client\": image_client,\n",
        "        \"router_agent\": router_agent,\n",
        "        \"router_chain\": router_chain,\n",
        "        \"session_id\": session_id,\n",
        "        \"scenario\": \"\"\n",
        "    }\n",
        "    return initial_state\n",
        "\n"
      ],
      "metadata": {
        "id": "kCd3A71v8vds"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "API Functions"
      ],
      "metadata": {
        "id": "3Ix_E_YWX9Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    # Load environment variables - in production use proper env management\n",
        "    groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not groq_api_key or not openai_api_key:\n",
        "        raise ValueError(\"API keys not found in environment variables\")\n",
        "\n",
        "    # Initialize agents\n",
        "    app.state.agents = initialize_agents(groq_api_key, openai_api_key)\n",
        "    app.state.graph, app.state.router_chain = create_image_dataset_graph(app.state.agents)\n",
        "\n",
        "    yield"
      ],
      "metadata": {
        "id": "zA4UpXAn8vaL"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize FastAPI app\n",
        "app = FastAPI(lifespan=lifespan)\n",
        "\n",
        "# Add CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Adjust this in production\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")"
      ],
      "metadata": {
        "id": "q8cfvLoMufLR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models for API\n",
        "class UserMessage(BaseModel):\n",
        "    message: str\n",
        "    session_id: str\n",
        "\n",
        "class AIResponse(BaseModel):\n",
        "    response: str\n",
        "    has_images: bool = False\n",
        "    image_urls: List[str] = []\n",
        "    session_id: str\n",
        "\n",
        "class RouterOutput(BaseModel):\n",
        "    \"\"\"Output schema for the router agent\"\"\"\n",
        "    agent: Literal[\"conversation_agent\", \"sample_image_agent\", \"dataset_generator_agent\"]\n",
        "    confidence: float = Field(description=\"Confidence level from 0.0 to 1.0\")\n",
        "    reasoning: str = Field(description=\"Brief explanation of why this agent was chosen\")\n"
      ],
      "metadata": {
        "id": "e7mEITgSpuj2"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API endpoints\n",
        "@app.post(\"/chat\", response_model=AIResponse)\n",
        "async def chat(user_message: UserMessage):\n",
        "    \"\"\"Process a user message using the LangGraph workflow\"\"\"\n",
        "    session_id = user_message.session_id\n",
        "    message = user_message.message\n",
        "\n",
        "    # Get or create session\n",
        "    if session_id not in active_sessions:\n",
        "        active_sessions[session_id] = create_new_session(session_id, app.state.agents, app.state.router_chain)\n",
        "\n",
        "    state = active_sessions[session_id]\n",
        "\n",
        "    # Add user message to state\n",
        "    state[\"messages\"].append(HumanMessage(content=message))\n",
        "\n",
        "    # Check for any feedback to sample images\n",
        "    if state[\"current_agent\"] == \"sample_image_agent\" and message:\n",
        "        state[\"user_feedback\"] = message\n",
        "\n",
        "    # Process message through the graph - always start at router\n",
        "    new_state = app.state.graph.invoke(state, {\"starting_node\": \"router\"})\n",
        "    active_sessions[session_id] = new_state\n",
        "\n",
        "    # Extract response for the API\n",
        "    ai_response = new_state[\"messages\"][-1].content\n",
        "\n",
        "    # Check if we have sample images to return\n",
        "    has_images = False\n",
        "    image_urls = []\n",
        "\n",
        "    if \"sample_images\" in new_state and new_state[\"sample_images\"]:\n",
        "        has_images = True\n",
        "        image_urls = new_state[\"sample_images\"]\n",
        "\n",
        "    # If we just completed dataset generation, include those images\n",
        "    if new_state[\"current_agent\"] == \"dataset_generator_agent\" and \"generate_dataset\" in str(new_state):\n",
        "        has_images = True\n",
        "        # Get the full list of generated images if available\n",
        "        # This would need to be implemented based on how you store the generated images\n",
        "\n",
        "    return AIResponse(\n",
        "        response=ai_response,\n",
        "        has_images=has_images,\n",
        "        image_urls=image_urls,\n",
        "        session_id=session_id\n",
        "    )\n"
      ],
      "metadata": {
        "id": "q6Wvg-LtoC7Y"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.get(\"/sessions/{session_id}\")\n",
        "async def get_session(session_id: str):\n",
        "    \"\"\"Get information about a specific session\"\"\"\n",
        "    if session_id not in active_sessions:\n",
        "        raise HTTPException(status_code=404, detail=\"Session not found\")\n",
        "\n",
        "    state = active_sessions[session_id]\n",
        "\n",
        "    # Extract conversation messages for the frontend\n",
        "    messages = []\n",
        "    for msg in state[\"messages\"]:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            messages.append({\"role\": \"user\", \"content\": msg.content})\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "\n",
        "    return {\n",
        "        \"session_id\": session_id,\n",
        "        \"messages\": messages,\n",
        "        \"current_agent\": state[\"current_agent\"],\n",
        "        \"has_samples\": len(state.get(\"sample_images\", [])) > 0\n",
        "    }"
      ],
      "metadata": {
        "id": "sm9rmWdloC3-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.post(\"/sessions/new\")\n",
        "async def create_session():\n",
        "    \"\"\"Create a new session\"\"\"\n",
        "    import uuid\n",
        "\n",
        "    session_id = str(uuid.uuid4())\n",
        "    active_sessions[session_id] = create_new_session(session_id, app.state.agents, app.state.router_chain)\n",
        "\n",
        "    return {\"session_id\": session_id}\n",
        "\n",
        "\n",
        "@app.delete(\"/sessions/{session_id}\")\n",
        "async def delete_session(session_id: str):\n",
        "    \"\"\"Delete a session\"\"\"\n",
        "    if session_id in active_sessions:\n",
        "        del active_sessions[session_id]\n",
        "\n",
        "    return {\"status\": \"deleted\", \"session_id\": session_id}\n"
      ],
      "metadata": {
        "id": "Y0t6KeZtoXTd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to run the server directly\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "id": "4KEZJapVoWrC",
        "outputId": "e271c2a8-bb1a-4916-94e7-d780f633f000",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "asyncio.run() cannot be called from a running event loop",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-9889c1afcf77>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0muvicorn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0muvicorn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"0.0.0.0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mMultiprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# pragma: full coverage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, sockets)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msockets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msockets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# fail fast with short traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    187\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hPO1ND-BoWnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bOHkuVv4oWlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mwGpV2fRoWie"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}