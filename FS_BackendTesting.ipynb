{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QeQLP_jNVz7j",
        "outputId": "586f9b33-92f3-40fc-fe6c-9b51bbf5f4cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.25-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.50)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.61-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.16)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.11.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.22.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting langchain-core<0.4,>=0.1 (from langgraph)\n",
            "  Downloading langchain_core-0.3.51-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.13.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m801.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langgraph-0.3.25-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.4/142.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading groq-0.22.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.51-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.3/423.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
            "Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.61-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m606.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: xxhash, python-dotenv, ormsgpack, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, langgraph-sdk, groq, dataclasses-json, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain_groq, langgraph-prebuilt, langchain, langgraph, langchain_community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.50\n",
            "    Uninstalling langchain-core-0.3.50:\n",
            "      Successfully uninstalled langchain-core-0.3.50\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.7\n",
            "    Uninstalling langchain-text-splitters-0.3.7:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.7\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.22\n",
            "    Uninstalling langchain-0.3.22:\n",
            "      Successfully uninstalled langchain-0.3.22\n",
            "Successfully installed dataclasses-json-0.6.7 groq-0.22.0 httpx-sse-0.4.0 langchain-0.3.23 langchain-core-0.3.51 langchain-text-splitters-0.3.8 langchain_community-0.3.21 langchain_groq-0.3.2 langgraph-0.3.25 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.61 marshmallow-3.26.1 mypy-extensions-1.0.0 ormsgpack-1.9.1 pydantic-settings-2.8.1 python-dotenv-1.1.0 typing-inspect-0.9.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph langsmith langchain langchain_groq langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('groq_api_key')"
      ],
      "metadata": {
        "id": "A9Qhe32kV21r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, TypedDict, Annotated, Sequence, Any\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "import operator\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "import json\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "import re\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing_extensions import Literal\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZKwhRMMvV23_",
        "outputId": "7259204f-be15-4152-8d43-4b9f9574c897"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphState(TypedDict):  # Class GraphState\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    current_agent: str\n",
        "    current_llm_model: str\n",
        "    context_summary: str\n",
        "\n",
        "state = StateGraph(GraphState)"
      ],
      "metadata": {
        "id": "Nx0uzJCFeIJy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_generator = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "system = \"\"\"You are a prompt generator for text to image model. Your task is to generate 50 diffrent prompts for text to image model to generate diverse images given to you\n",
        "the scenarios. example if I give you scenario as 'people in distress in the context to disaster' you give me different prompts for image generator model to\n",
        "generate a complete dataset of images of people in distress covering diverse scenarios. \"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt_generator_agent = prompt | prompt_generator\n",
        "\n",
        "def prompt_generator_handler(state: GraphState):\n",
        "    \"\"\" The marketing RAG powered Agent to resolve user queries\"\"\"\n",
        "\n",
        "    question = state[\"messages\"][-1].content\n",
        "\n",
        "    inbuilt_query = f\"\"\" You are a prompt generator for a text-to-image model. Given the user scenario and access to the full conversation history,\n",
        "your task is to generate 50 diverse and creative image generation prompts based on the scenario provided by the user.\n",
        "Make sure the prompts cover a wide range of sub-scenarios, perspectives, environments, and subjects to create a comprehensive dataset of images.\n",
        "what i need as the output exactly is: return the python list of 50 prompt strings.\n",
        "\n",
        "User scenario:\n",
        "{question}\n",
        "\n",
        "    \"\"\"\n",
        "    response = prompt_generator_agent.invoke(inbuilt_query)\n",
        "\n",
        "    # Add the question-response pair to conversation_history\n",
        "    conversation_id = len(state[\"conversation_history\"]) + 1\n",
        "    state[\"conversation_history\"][conversation_id] = {\n",
        "        \"question\": question,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    state[\"llm_model\"] = \"Gemma2-9b-It\"\n",
        "\n",
        "    # Also add the AI response to the messages list\n",
        "    state[\"messages\"].append(AIMessage(content=response.content))\n",
        "\n",
        "    # Update the current_agent within the state dictionary\n",
        "    state['current_agent'] = \"prompt_generator_agent\"  # Or the appropriate agent name\n",
        "\n",
        "    # Return the updated state\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "CdiqxrarV26F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_generator = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "system = \"\"\"You are a prompt generator for text to image model. Your task is to generate 50 diffrent prompts for text to image model to generate diverse images given to you\n",
        "the scenarios. example if I give you scenario as 'people in distress in the context to disaster' you give me different prompts for image generator model to\n",
        "generate a complete dataset of images of people in distress covering diverse scenarios. \"\"\"\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chat_agent = chat_prompt | chat_generator\n",
        "\n",
        "def chat_handler(state: GraphState):\n",
        "    \"\"\" The marketing RAG powered Agent to resolve user queries\"\"\"\n",
        "\n",
        "    question = state[\"messages\"][-1].content\n",
        "\n",
        "    inbuilt_query = f\"\"\" You are a prompt generator for a text-to-image model. Given the user scenario and access to the full conversation history,\n",
        "your task is to generate 50 diverse and creative image generation prompts based on the scenario provided by the user.\n",
        "Make sure the prompts cover a wide range of sub-scenarios, perspectives, environments, and subjects to create a comprehensive dataset of images.\n",
        "\n",
        "User scenario:\n",
        "{question}\n",
        "    \"\"\"\n",
        "    response = prompt_generator_agent.invoke(inbuilt_query)\n",
        "\n",
        "    # Add the question-response pair to conversation_history\n",
        "    conversation_id = len(state[\"conversation_history\"]) + 1\n",
        "    state[\"conversation_history\"][conversation_id] = {\n",
        "        \"question\": question,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    state[\"llm_model\"] = \"Gemma2-9b-It\"\n",
        "\n",
        "    # Also add the AI response to the messages list\n",
        "    state[\"messages\"].append(AIMessage(content=response.content))\n",
        "\n",
        "    # Update the current_agent within the state dictionary\n",
        "    state['current_agent'] = \"chat_agent\"  # Or the appropriate agent name\n",
        "\n",
        "    # Return the updated state\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "Silkk4JBG0bI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Initialize the state\n",
        "    initial_state = {\n",
        "        \"messages\": [],\n",
        "        \"current_agent\": \"prompt_generator_agent\",  # Set initial agent\n",
        "        \"current_llm_model\": \"deepseek-r1-distill-qwen-32b\",\n",
        "        \"context_summary\": \"\",\n",
        "        \"conversation_history\": {} # Initialize conversation history\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"Enter a scenario (or type 'exit' to quit): \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Create a HumanMessage from the input\n",
        "        initial_state[\"messages\"].append(HumanMessage(content=user_input))\n",
        "\n",
        "        # Invoke the prompt_generator_handler\n",
        "        updated_state = prompt_generator_handler(initial_state)\n",
        "\n",
        "        # Print or process the updated state's AI response\n",
        "        print(updated_state[\"messages\"][-1].content)\n",
        "\n",
        "        # Update the initial state with the new state for the next iteration\n",
        "        initial_state = updated_state\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RS302m6eVGU",
        "outputId": "f2ce156b-3e69-452c-fca2-4f73b4e0af40"
      },
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a scenario (or type 'exit' to quit): carssss\n",
            "## 50 Image Prompts for \"Cars\"\n",
            "\n",
            "1.  A vintage red convertible speeding down a desert highway at sunset.\n",
            "2.  A sleek, futuristic electric car parked on a moonlit rooftop cityscape.\n",
            "3.  A family of four piled into a dusty, old station wagon, driving through a rural countryside.\n",
            "4.  A lone motorcycle rider disappearing into a dense forest.\n",
            "5.  A futuristic flying car navigating a crowded, neon-lit city sky.\n",
            "6.  A child's toy car painted with vibrant colors, sitting on a dusty attic floor.\n",
            "7.  A rusty abandoned car rusting in a junkyard, overgrown with weeds.\n",
            "8.  A classic muscle car, roaring its engine in a smoky drag race.\n",
            "9.  A group of friends laughing in a customized van, driving to a music festival.\n",
            "10. A heavily armored military vehicle driving through a war-torn city.\n",
            "11. A pristine, white limousine pulling up to a red carpet premiere.\n",
            "12. A tiny, remote-controlled car racing around a miniature track.\n",
            "13. A car crash scene with shattered glass and twisted metal.\n",
            "14. A car submerged in a flooded street, with water reaching the windows.\n",
            "15. A group of commuters stuck in a long, gridlocked traffic jam.\n",
            "16. A car driving through a thick, white snowstorm.\n",
            "17. A car parked on a scenic mountain road, overlooking a breathtaking view.\n",
            "18. A vintage car parked outside a charming diner, with steam rising from a cup of coffee.\n",
            "19. A car race taking place on an icy racetrack, with cars sliding and spinning.\n",
            "20. A futuristic self-driving car navigating a smart city.\n",
            "21. A car being towed away by a flatbed truck.\n",
            "22. A car being lifted by a crane, being inspected for damage.\n",
            "23. A car wash with soapy water spraying and drying a shiny vehicle.\n",
            "24. A car mechanic working under the hood of a car, using tools.\n",
            "25. A car interior with a driver focused on the road, music playing in the background.\n",
            "26. A car trunk filled with luggage, ready for a road trip.\n",
            "27. A car parked in front of a beautiful beach house, with people enjoying the sand.\n",
            "28. A car driving through a dense jungle, with lush greenery and exotic animals.\n",
            "29. A car driving through a foggy desert, with the sun barely visible.\n",
            "30. A car parked in a bustling city street, surrounded by pedestrians and shops.\n",
            "31. A car driving through a tunnel, with light emanating from the other end.\n",
            "32. A car parked on a cliffside, overlooking a dramatic ocean view.\n",
            "33. A car driving through a field of sunflowers, with a golden light illuminating the scene.\n",
            "34. A car driving through a winter wonderland, with snow-covered trees and frozen lakes.\n",
            "35. A car parked in a deserted parking lot, with a lone figure walking towards it.\n",
            "36. A car driving through a crowded marketplace, with vendors selling their wares.\n",
            "37. A car driving through a nighttime cityscape, with neon lights reflecting on the wet streets.\n",
            "38. A car driving in reverse, trying to navigate a tight space.\n",
            "39. A car parked in front of a hospital, with people rushing in and out.\n",
            "40. A car driving on a narrow mountain road, with a sheer drop-off on one side.\n",
            "41. A car parked in a cemetery, with tombstones and a sense of solemnity.\n",
            "42. A car driving through a historical district, with old buildings and cobblestone streets.\n",
            "43. A car parked in front of a school, with children playing outside.\n",
            "44. A car driving through a forest, with sunlight filtering through the trees.\n",
            "45. A car driving through a thunderstorm, with lightning flashing and thunder booming.\n",
            "46. A car parked in a field, surrounded by wildflowers.\n",
            "47. A car driving through a construction site, with workers and machinery.\n",
            "48. A car driving through a haunted house, with spooky decorations and eerie sounds.\n",
            "49. A car driving through a dreamlike landscape, with surreal colors and shapes.\n",
            "50. A car driving through a time portal, entering a different era.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Enter a scenario (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import requests\n",
        "import os\n",
        "import ast\n",
        "from time import sleep\n",
        "\n",
        "# Set OpenAI API key from Colab's env\n",
        "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "openai.api_key = userdata.get('groq_api_key')\n",
        "# Directory to save images\n",
        "output_dir = \"/content/generated_images\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 🔁 STEP 1: Invoke your prompt generator agent ===\n",
        "scenario = \"realistic cinematic images of supercars in different scenarios\"\n",
        "agent_output = prompt_generator_agent.invoke(scenario)\n",
        "\n",
        "# === 🧠 STEP 2: Parse output string to Python list ===\n",
        "# agent_output.content should be a string like: '[\"prompt1\", \"prompt2\", ...]'\n",
        "try:\n",
        "    prompt_list = ast.literal_eval(agent_output.content)\n",
        "    if not isinstance(prompt_list, list):\n",
        "        raise ValueError(\"Parsed output is not a list.\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Failed to parse prompts from agent: {e}\")\n",
        "\n",
        "print(f\"✅ Got {len(prompt_list)} prompts from agent!\")\n",
        "\n",
        "# === 🎨 STEP 3: Generate and save images ===\n",
        "for idx, prompt in enumerate(prompt_list):\n",
        "    try:\n",
        "        print(f\"[{idx+1}/{len(prompt_list)}] Generating image for prompt: {prompt}\")\n",
        "\n",
        "        response = openai.Image.create(\n",
        "            model=\"dall-e-3\",\n",
        "            prompt=prompt,\n",
        "            size=\"1024x1024\",\n",
        "            n=1,\n",
        "            response_format=\"url\"\n",
        "        )\n",
        "\n",
        "        image_url = response[\"data\"][0][\"url\"]\n",
        "        image_data = requests.get(image_url).content\n",
        "        file_path = os.path.join(output_dir, f\"image_{idx+1:02}.png\")\n",
        "\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(image_data)\n",
        "\n",
        "        print(f\"✅ Saved: {file_path}\")\n",
        "        sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error with prompt {idx+1}: {e}\")\n"
      ],
      "metadata": {
        "id": "pVNemTlLV28W",
        "outputId": "0f7a64cc-36d6-4a2e-b61b-4275a5335563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Failed to parse prompts from agent: invalid decimal literal (<unknown>, line 7)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSyntaxError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-cb63308e1c8d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprompt_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ast.py\u001b[0m in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mnode_or_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ast.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, filename, mode, type_comments, feature_version)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Else it should be an int giving the minor version for 3.x.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     return compile(source, filename, mode, flags,\n\u001b[0m\u001b[1;32m     51\u001b[0m                    _feature_version=feature_version)\n",
            "\u001b[0;31mSyntaxError\u001b[0m: invalid decimal literal (<unknown>, line 7)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-cb63308e1c8d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Parsed output is not a list.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to parse prompts from agent: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✅ Got {len(prompt_list)} prompts from agent!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to parse prompts from agent: invalid decimal literal (<unknown>, line 7)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NcZdo9WeV2-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IoS_BAAJV3B5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}