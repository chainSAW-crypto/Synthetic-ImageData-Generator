{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QeQLP_jNVz7j",
        "outputId": "970b779a-bcc3-4475-fa4f-2530ae382663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.27-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.50)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.61-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.16)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.11.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.22.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting langchain-core<0.4,>=0.1 (from langgraph)\n",
            "  Downloading langchain_core-0.3.51-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.13.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langgraph-0.3.27-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading groq-0.22.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.51-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.3/423.3 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
            "Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.61-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: xxhash, python-dotenv, ormsgpack, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, langgraph-sdk, groq, dataclasses-json, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain_groq, langgraph-prebuilt, langchain, langgraph, langchain_community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.50\n",
            "    Uninstalling langchain-core-0.3.50:\n",
            "      Successfully uninstalled langchain-core-0.3.50\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.7\n",
            "    Uninstalling langchain-text-splitters-0.3.7:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.7\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.22\n",
            "    Uninstalling langchain-0.3.22:\n",
            "      Successfully uninstalled langchain-0.3.22\n",
            "Successfully installed dataclasses-json-0.6.7 groq-0.22.0 httpx-sse-0.4.0 langchain-0.3.23 langchain-core-0.3.51 langchain-text-splitters-0.3.8 langchain_community-0.3.21 langchain_groq-0.3.2 langgraph-0.3.27 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.61 marshmallow-3.26.1 mypy-extensions-1.0.0 ormsgpack-1.9.1 pydantic-settings-2.8.1 python-dotenv-1.1.0 typing-inspect-0.9.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph langsmith langchain langchain_groq langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('groq_api_key')"
      ],
      "metadata": {
        "id": "A9Qhe32kV21r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, TypedDict, Annotated, Sequence, Any\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "import operator\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "import json\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "import re\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing_extensions import Literal\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZKwhRMMvV23_",
        "outputId": "df09a7d2-8670-4b2f-c416-9fad38c74347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphState(TypedDict):  # Class GraphState\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    current_agent: str\n",
        "    current_llm_model: str\n",
        "    context_summary: str\n",
        "\n",
        "state = StateGraph(GraphState)"
      ],
      "metadata": {
        "id": "Nx0uzJCFeIJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_generator = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "system = \"\"\"You are a prompt generator for text to image model. Your task is to generate 50 diffrent prompts for text to image model to generate diverse images given to you\n",
        "the scenarios. example if I give you scenario as 'people in distress in the context to disaster' you give me different prompts for image generator model to\n",
        "generate a complete dataset of images of people in distress covering diverse scenarios. \"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt_generator_agent = prompt | prompt_generator\n",
        "\n",
        "def prompt_generator_handler(state: GraphState):\n",
        "    \"\"\" The marketing RAG powered Agent to resolve user queries\"\"\"\n",
        "\n",
        "    question = state[\"messages\"][-1].content\n",
        "\n",
        "    inbuilt_query = f\"\"\" You are a prompt generator for a text-to-image model. Given the user scenario and access to the full conversation history,\n",
        "your task is to generate 50 diverse and creative image generation prompts based on the scenario provided by the user.\n",
        "Make sure the prompts cover a wide range of sub-scenarios, perspectives, environments, and subjects to create a comprehensive dataset of images.\n",
        "what i need as the output exactly is: return the python list of 50 prompt strings. agent_output.content should be a string like: '[\"prompt1\", \"prompt2\", ...]'\n",
        "\n",
        "User scenario:\n",
        "{question}\n",
        "\n",
        "    \"\"\"\n",
        "    response = prompt_generator_agent.invoke(inbuilt_query)\n",
        "\n",
        "    # Add the question-response pair to conversation_history\n",
        "    conversation_id = len(state[\"conversation_history\"]) + 1\n",
        "    state[\"conversation_history\"][conversation_id] = {\n",
        "        \"question\": question,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    state[\"llm_model\"] = \"Gemma2-9b-It\"\n",
        "\n",
        "    # Also add the AI response to the messages list\n",
        "    state[\"messages\"].append(AIMessage(content=response.content))\n",
        "\n",
        "    # Update the current_agent within the state dictionary\n",
        "    state['current_agent'] = \"prompt_generator_agent\"  # Or the appropriate agent name\n",
        "\n",
        "    # Return the updated state\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "CdiqxrarV26F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_generator = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "system = \"\"\"You are a prompt generator for text to image model. Your task is to generate 50 diffrent prompts for text to image model to generate diverse images given to you\n",
        "the scenarios. example if I give you scenario as 'people in distress in the context to disaster' you give me different prompts for image generator model to\n",
        "generate a complete dataset of images of people in distress covering diverse scenarios. \"\"\"\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chat_agent = chat_prompt | chat_generator\n",
        "\n",
        "def chat_handler(state: GraphState):\n",
        "    \"\"\" The marketing RAG powered Agent to resolve user queries\"\"\"\n",
        "\n",
        "    question = state[\"messages\"][-1].content\n",
        "\n",
        "    inbuilt_query = f\"\"\" You are a prompt generator for a text-to-image model. Given the user scenario and access to the full conversation history,\n",
        "your task is to generate 50 diverse and creative image generation prompts based on the scenario provided by the user.\n",
        "Make sure the prompts cover a wide range of sub-scenarios, perspectives, environments, and subjects to create a comprehensive dataset of images.\n",
        "\n",
        "User scenario:\n",
        "{question}\n",
        "    \"\"\"\n",
        "    response = prompt_generator_agent.invoke(inbuilt_query)\n",
        "\n",
        "    # Add the question-response pair to conversation_history\n",
        "    conversation_id = len(state[\"conversation_history\"]) + 1\n",
        "    state[\"conversation_history\"][conversation_id] = {\n",
        "        \"question\": question,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    state[\"llm_model\"] = \"Gemma2-9b-It\"\n",
        "\n",
        "    # Also add the AI response to the messages list\n",
        "    state[\"messages\"].append(AIMessage(content=response.content))\n",
        "\n",
        "    # Update the current_agent within the state dictionary\n",
        "    state['current_agent'] = \"chat_agent\"  # Or the appropriate agent name\n",
        "\n",
        "    # Return the updated state\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "Silkk4JBG0bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Initialize the state\n",
        "    initial_state = {\n",
        "        \"messages\": [],\n",
        "        \"current_agent\": \"prompt_generator_agent\",  # Set initial agent\n",
        "        \"current_llm_model\": \"deepseek-r1-distill-qwen-32b\",\n",
        "        \"context_summary\": \"\",\n",
        "        \"conversation_history\": {} # Initialize conversation history\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"Enter a scenario (or type 'exit' to quit): \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Create a HumanMessage from the input\n",
        "        initial_state[\"messages\"].append(HumanMessage(content=user_input))\n",
        "\n",
        "        # Invoke the prompt_generator_handler\n",
        "        updated_state = prompt_generator_handler(initial_state)\n",
        "\n",
        "        # Print or process the updated state's AI response\n",
        "        print(updated_state[\"messages\"][-1].content)\n",
        "\n",
        "        # Update the initial state with the new state for the next iteration\n",
        "        initial_state = updated_state\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RS302m6eVGU",
        "outputId": "8d0da30f-c6b9-419a-acfe-34de56cea3f2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a scenario (or type 'exit' to quit): Birds\n",
            "```python\n",
            "agent_output = [\n",
            "    \"A majestic bald eagle soaring above snow-capped mountains at sunset\",\n",
            "    \"A hummingbird hovering in mid-air, sipping nectar from a vibrant flower\",\n",
            "    \"A flock of colorful parrots squawking in a lush rainforest canopy\",\n",
            "    \"A lone penguin waddling across a frozen Antarctic landscape\",\n",
            "    \"A pair of lovebirds perched on a tree branch, cuddling each other\",\n",
            "    \"A tiny wren singing a beautiful melody in a dew-kissed garden\",\n",
            "    \"A peacock spreading its iridescent tail feathers in a display of grandeur\",\n",
            "    \"A group of owls perched on branches, their glowing eyes observing the night\",\n",
            "    \"A majestic owl with its wings spread wide, silhouetted against a full moon\",\n",
            "    \"A cardinal perched on a snow-covered branch, its bright red plumage contrasting with the white\",\n",
            "    \"A flock of starlings flying in a mesmerizing V-formation across a clear blue sky\",\n",
            "    \"A blue jay perched on a fence post, its blue and white feathers shimmering in the sunlight\",\n",
            "    \"A woodpecker hammering away at a tree trunk, its red crest bobbing\",\n",
            "    \"A hawk soaring high above the ground, its keen eyes searching for prey\",\n",
            "    \"A family of ducks swimming in a calm pond, their ducklings trailing behind\",\n",
            "    \"A swan gliding gracefully across a lake, its white feathers reflecting the sunlight\",\n",
            "    \"A goose honking loudly, flying in formation with its flock\",\n",
            "    \"A vulture circling high above a vast desert landscape\",\n",
            "    \"A robin building a nest in a tree branch\",\n",
            "    \"A sparrow hopping on the ground, searching for seeds\",\n",
            "    \"A hummingbird feeding on a flower, its wings beating rapidly\",\n",
            "    \"A parrot mimicking the sounds of a human voice\",\n",
            "    \"A crow perched on a rooftop, its dark feathers blending with the shadows\",\n",
            "    \"A pigeon landing on a statue in a city park\",\n",
            "    \"A flock of geese flying south for the winter\",\n",
            "    \"A snowy owl perched on a snowdrift, its white plumage camouflaged against the snowy backdrop\",\n",
            "    \"A bluebird singing in a tree\",\n",
            "    \"A butterfly perched on a bird's beak\",\n",
            "    \"A bird's nest filled with eggs\",\n",
            "    \"A bird flying through a thunderstorm\",\n",
            "    \"A bird's eye view of a forest\",\n",
            "    \"A bird's feather close-up\",\n",
            "    \"A bird skeleton\",\n",
            "    \"An abstract representation of a bird in flight\",\n",
            "    \"A bird made entirely of flowers\",\n",
            "    \"A surreal landscape with giant birds flying overhead\",\n",
            "    \"A dystopian city where birds are extinct\",\n",
            "    \"A dreamlike scene with birds transforming into other creatures\",\n",
            "    \"A bird symbolizing freedom and hope\",\n",
            "    \"A bird representing loneliness and isolation\",\n",
            "    \"A bird representing wisdom and knowledge\",\n",
            "    \"A bird representing beauty and grace\",\n",
            "    \"A bird representing danger and fear\",\n",
            "    \"A bird representing the cycle of life and death\",\n",
            "    \"A bird representing the connection between humans and nature\"\n",
            "]\n",
            "\n",
            "```\n",
            "\n",
            "Enter a scenario (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import requests\n",
        "import os\n",
        "import ast\n",
        "from time import sleep\n",
        "\n",
        "# Set OpenAI API key from Colab's env\n",
        "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "# Directory to save images\n",
        "output_dir = \"/content/generated_images\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "initial_state = {\n",
        "        \"messages\": [],\n",
        "        \"current_agent\": \"prompt_generator_agent\",  # Set initial agent\n",
        "        \"current_llm_model\": \"deepseek-r1-distill-qwen-32b\",\n",
        "        \"context_summary\": \"\",\n",
        "        \"conversation_history\": {} # Initialize conversation history\n",
        "    }\n",
        "\n",
        "# === ðŸ” STEP 1: Invoke your prompt generator agent ===\n",
        "scenario = input(\"Enter a scenario (or type 'exit' to quit): \")\n",
        "# agent_output = prompt_generator_agent.invoke(scenario)\n",
        "\n",
        "initial_state[\"messages\"].append(HumanMessage(content=scenario))\n",
        "\n",
        "# Invoke the prompt_generator_handler\n",
        "updated_state = prompt_generator_handler(initial_state)\n",
        "\n",
        "# Print or process the updated state's AI response\n",
        "agent_output = updated_state[\"messages\"][-1].content\n",
        "print(updated_state[\"messages\"][-1].content)\n",
        "\n",
        "# Update the initial state with the new state for the next iteration\n",
        "initial_state = updated_state\n",
        "\n",
        "\n",
        "# === ðŸ§  STEP 2: Parse output string to Python list ===\n",
        "# agent_output.content should be a string like: '[\"prompt1\", \"prompt2\", ...]'\n",
        "\n",
        "\n",
        "try:\n",
        "    # Attempt to parse as JSON first\n",
        "    prompt_list = json.loads(agent_output)\n",
        "\n",
        "    # If JSON parsing fails, fallback to ast.literal_eval\n",
        "except json.JSONDecodeError:\n",
        "    try:\n",
        "        prompt_list = ast.literal_eval(agent_output)\n",
        "    except (SyntaxError, ValueError) as e:\n",
        "        # If both JSON and literal_eval fail, extract prompts using regex\n",
        "        prompt_pattern = r'\"(.*?)\"'  # Matches text within double quotes\n",
        "        prompt_list = re.findall(prompt_pattern, agent_output)\n",
        "        if not prompt_list:\n",
        "            raise ValueError(f\"Failed to extract prompts using regex: {e}\")\n",
        "\n",
        "    if not isinstance(prompt_list, list):\n",
        "        raise ValueError(\"Parsed output is not a list.\")\n",
        "\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Failed to parse prompts from agent: {e}\")\n",
        "\n",
        "\n",
        "print(f\"âœ… Got {len(prompt_list)} prompts from agent!\")\n",
        "\n",
        "# === ðŸŽ¨ STEP 3: Generate and save images ===\n",
        "client = openai.OpenAI(\n",
        "    api_key= userdata.get(\"OPENAI_API_KEY\")\n",
        ")\n",
        "for idx, prompt in enumerate(prompt_list):\n",
        "    try:\n",
        "        print(f\"[{idx+1}/{len(prompt_list)}] Generating image for prompt: {prompt}\")\n",
        "\n",
        "        response = client.images.generate(\n",
        "            model=\"dall-e-2\",  # or \"dall-e-2\"\n",
        "            prompt=prompt,\n",
        "            size=\"256x256\",\n",
        "            quality=\"standard\",\n",
        "            n=1\n",
        "        )\n",
        "\n",
        "        image_url = response[\"data\"][0][\"url\"]\n",
        "        image_data = requests.get(image_url).content\n",
        "        file_path = os.path.join(output_dir, f\"image_{idx+1:02}.png\")\n",
        "\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(image_data)\n",
        "\n",
        "        print(f\"âœ… Saved: {file_path}\")\n",
        "        sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error with prompt {idx+1}: {e}\")\n"
      ],
      "metadata": {
        "id": "pVNemTlLV28W",
        "outputId": "da0201da-c270-473e-8039-6af3bf6e9cb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a scenario (or type 'exit' to quit): trees\n",
            "```python\n",
            "[\"A towering redwood sequoia bathed in dappled sunlight, towering over a lush forest floor.\", \n",
            "\"A lone, gnarled oak tree standing sentinel on a windswept hilltop, branches reaching towards the stormy sky.\",\n",
            "\"A grove of ancient olive trees, their silvery leaves shimmering in the Mediterranean sun.\",\n",
            "\"A vibrant Japanese maple, ablaze with autumn colors, leaves cascading down like fiery rain.\",\n",
            "\"Twisted and ancient cypress trees lining a foggy Italian path, creating an eerie and mysterious atmosphere.\",\n",
            "\"A young sapling, its delicate leaves unfurling in the spring breeze, surrounded by wildflowers.\",\n",
            "\"A dense forest of towering pine trees, their needles carpeting the ground in a thick layer of green.\",\n",
            "\"A panoramic view of a sprawling forest, sunlight filtering through the leaves, creating a mosaic of light and shadow.\",\n",
            "\"A close-up shot of a tree bark, showcasing its intricate patterns and textures.\",\n",
            "\"A cluster of birch trees, their white bark glowing in the moonlight, casting long shadows on the forest floor.\",\n",
            "\"A deciduous forest in full bloom, flowers carpeting the ground beneath the canopy of leaves.\",\n",
            "\"An abstract representation of a tree, using lines and shapes to capture its essence.\",\n",
            "\"A tree silhouette against a fiery sunset, casting a long, dramatic shadow.\",\n",
            "\"A surreal image of a tree growing through a cityscape, its roots reaching towards the sky.\",\n",
            "\"A whimsical illustration of a treehouse nestled amongst the branches of a giant oak.\",\n",
            "\"A magical forest scene, with glowing mushrooms and bioluminescent trees.\",\n",
            "\"A winter wonderland, with snow-covered trees and frozen lakes.\",\n",
            "\"A dense jungle, with towering trees and exotic flowers.\",\n",
            "\"A desert oasis, with a lone palm tree providing shade and water.\",\n",
            "\"A futuristic cityscape, with towering trees incorporated into the architecture.\",\n",
            "\"A tree with glowing, bioluminescent leaves, illuminating the forest at night.\",\n",
            "\"A tree growing out of a cracked earth, symbolizing resilience and hope.\",\n",
            "\"A tree with roots reaching deep into the ocean, connecting to the depths of the earth.\",\n",
            "\"A tree with fruit that resembles human faces, creating a sense of surrealism.\",\n",
            "\"A tree with branches that transform into birds in flight.\",\n",
            "\"A tree with leaves that change color with the viewer's emotions.\",\n",
            "\"A tree with its branches forming the shape of a human figure.\",\n",
            "\"A tree with its roots forming a labyrinthine maze.\",\n",
            "\"A tree with its trunk covered in ancient hieroglyphics.\",\n",
            "\"A tree with its leaves forming a protective canopy over a village below.\",\n",
            "\"A tree with its branches stretching towards the stars, symbolizing aspirations and dreams.\",\n",
            "\"A tree sculpted from ice, shimmering in the sunlight.\",\n",
            "\"A tree made entirely of flowers, blooming in all colors.\",\n",
            "\"A tree with its roots forming a giant heart, symbolizing love and connection.\",\n",
            "\"A tree with its leaves whispering secrets to the wind.\",\n",
            "\"A tree with its branches reaching out to embrace the world.\",\n",
            "\"A tree with its trunk glowing with an ethereal light.\",\n",
            "\"A tree that transcends the boundaries of reality, existing in multiple dimensions.\",\n",
            "\"A tree that represents the cycle of life, death, and rebirth.\"] \n",
            "``` \n",
            "\n",
            "\n",
            "\n",
            "âœ… Got 39 prompts from agent!\n",
            "[1/39] Generating image for prompt: A towering redwood sequoia bathed in dappled sunlight, towering over a lush forest floor.\n",
            "âŒ Error with prompt 1: Error code: 400 - {'error': {'code': 'billing_hard_limit_reached', 'message': 'Billing hard limit has been reached', 'param': None, 'type': 'invalid_request_error'}}\n",
            "[2/39] Generating image for prompt: A lone, gnarled oak tree standing sentinel on a windswept hilltop, branches reaching towards the stormy sky.\n",
            "âŒ Error with prompt 2: Error code: 400 - {'error': {'code': 'billing_hard_limit_reached', 'message': 'Billing hard limit has been reached', 'param': None, 'type': 'invalid_request_error'}}\n",
            "[3/39] Generating image for prompt: A grove of ancient olive trees, their silvery leaves shimmering in the Mediterranean sun.\n",
            "âŒ Error with prompt 3: Error code: 400 - {'error': {'code': 'billing_hard_limit_reached', 'message': 'Billing hard limit has been reached', 'param': None, 'type': 'invalid_request_error'}}\n",
            "[4/39] Generating image for prompt: A vibrant Japanese maple, ablaze with autumn colors, leaves cascading down like fiery rain.\n",
            "âŒ Error with prompt 4: Error code: 400 - {'error': {'code': 'billing_hard_limit_reached', 'message': 'Billing hard limit has been reached', 'param': None, 'type': 'invalid_request_error'}}\n",
            "[5/39] Generating image for prompt: Twisted and ancient cypress trees lining a foggy Italian path, creating an eerie and mysterious atmosphere.\n",
            "âŒ Error with prompt 5: Error code: 400 - {'error': {'code': 'billing_hard_limit_reached', 'message': 'Billing hard limit has been reached', 'param': None, 'type': 'invalid_request_error'}}\n",
            "[6/39] Generating image for prompt: A young sapling, its delicate leaves unfurling in the spring breeze, surrounded by wildflowers.\n",
            "âŒ Error with prompt 6: Error code: 429 - {'error': {'code': 'rate_limit_exceeded', 'message': 'Rate limit exceeded for images per minute in organization org-t80GchCytxtJrChnmmxg6lXm. Limit: 5/1min. Current: 6/1min. Please visit https://platform.openai.com/docs/guides/rate-limits to learn how to increase your rate limit.', 'param': None, 'type': 'requests'}}\n",
            "[7/39] Generating image for prompt: A dense forest of towering pine trees, their needles carpeting the ground in a thick layer of green.\n",
            "âŒ Error with prompt 7: Error code: 429 - {'error': {'code': 'rate_limit_exceeded', 'message': 'Rate limit exceeded for images per minute in organization org-t80GchCytxtJrChnmmxg6lXm. Limit: 5/1min. Current: 6/1min. Please visit https://platform.openai.com/docs/guides/rate-limits to learn how to increase your rate limit.', 'param': None, 'type': 'requests'}}\n",
            "[8/39] Generating image for prompt: A panoramic view of a sprawling forest, sunlight filtering through the leaves, creating a mosaic of light and shadow.\n",
            "âŒ Error with prompt 8: Error code: 429 - {'error': {'code': 'rate_limit_exceeded', 'message': 'Rate limit exceeded for images per minute in organization org-t80GchCytxtJrChnmmxg6lXm. Limit: 5/1min. Current: 6/1min. Please visit https://platform.openai.com/docs/guides/rate-limits to learn how to increase your rate limit.', 'param': None, 'type': 'requests'}}\n",
            "[9/39] Generating image for prompt: A close-up shot of a tree bark, showcasing its intricate patterns and textures.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# thrown on 4xx and 5xx status code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/images/generations'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# thrown on 4xx and 5xx status code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/images/generations'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-d40bdfa72595>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[{idx+1}/{len(prompt_list)}] Generating image for prompt: {prompt}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         response = client.images.generate(\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dall-e-2\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# or \"dall-e-2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/images.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompt, model, n, quality, response_format, size, style, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \"\"\"\n\u001b[0;32m--> 264\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;34m\"/images/generations\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1009\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1058\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1009\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;31m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;31m# different thread if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         return self._request(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph langsmith langchain langchain_groq langchain_community langchain_openai FastAPI uvicorn"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unTgriWUuYMw",
        "outputId": "961a607d-99a4-4a35-b1cb-c37e2ee15ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.29-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting FastAPI\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.51)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.61-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.16)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.11.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.22.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.70.0)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from FastAPI)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from FastAPI) (4.13.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langgraph-0.3.29-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading groq-0.22.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.61-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: xxhash, uvicorn, python-dotenv, ormsgpack, mypy-extensions, marshmallow, httpx-sse, typing-inspect, tiktoken, starlette, pydantic-settings, langgraph-sdk, groq, FastAPI, dataclasses-json, langgraph-checkpoint, langchain_openai, langchain_groq, langgraph-prebuilt, langgraph, langchain_community\n",
            "Successfully installed FastAPI-0.115.12 dataclasses-json-0.6.7 groq-0.22.0 httpx-sse-0.4.0 langchain_community-0.3.21 langchain_groq-0.3.2 langchain_openai-0.3.12 langgraph-0.3.29 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.61 marshmallow-3.26.1 mypy-extensions-1.0.0 ormsgpack-1.9.1 pydantic-settings-2.8.1 python-dotenv-1.1.0 starlette-0.46.1 tiktoken-0.9.0 typing-inspect-0.9.0 uvicorn-0.34.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('groq_api_key')"
      ],
      "metadata": {
        "id": "bbUcySGzymz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Dict, List, Any, Optional, Literal\n",
        "from fastapi import FastAPI, HTTPException, Body, Depends\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_openai import OpenAI\n",
        "import openai\n",
        "from langgraph.graph import StateGraph, END\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from contextlib import asynccontextmanager"
      ],
      "metadata": {
        "id": "NcZdo9WeV2-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphState(TypedDict):\n",
        "    messages: List[Any]  # The messages passed between user and assistant\n",
        "    conversation_history: Dict  # A record of all conversation threads\n",
        "    current_agent: str  # Track which agent is currently active\n",
        "    prompt_list: List[str]  # Store generated prompts\n",
        "    sample_images: List[str]  # Store URLs of sample images\n",
        "    user_feedback: str  # Store user feedback on sample images\n",
        "    dataset_ready: bool  # Flag to indicate if user wants the full dataset\n",
        "    llm_model: str  # Track which LLM model is being used\n",
        "    session_id: str  # Unique identifier for the conversation session\n",
        "    scenario: str   # We'll store the scenarion and update it accordingly"
      ],
      "metadata": {
        "id": "IoS_BAAJV3B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variables to store active sessions\n",
        "active_sessions = {}\n",
        "\n",
        "def initialize_agents(groq_api_key, openai_api_key):  # Add initial_scenario argument\n",
        "    \"\"\"Initialize all the agent components\"\"\"\n",
        "    conversation_agent = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "    prompt_generator = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "    image_client = openai.OpenAI(api_key=openai_api_key)\n",
        "\n",
        "    # Router agent\n",
        "    router_agent = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "    return conversation_agent, prompt_generator, image_client, router_agent\n",
        "\n",
        "\n",
        "\n",
        "def init_img_specs(state, num, dimensions, initial_scenario):\n",
        "  state['scenario'] = initial_scenario\n",
        "\n",
        "  # also set dimansions and number of images\n"
      ],
      "metadata": {
        "id": "kJE2h6VxufXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Helper Functions"
      ],
      "metadata": {
        "id": "XEdQRcYGznfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def extract_scenario(state):\n",
        "#     \"\"\"Extract the user's scenario from the conversation history\"\"\"\n",
        "#     relevant_messages = []\n",
        "\n",
        "#     for message in state[\"messages\"]:\n",
        "#         if isinstance(message, HumanMessage):\n",
        "#             relevant_messages.append(message.content)\n",
        "\n",
        "#     # Concatenate all user messages into a single scenario description\n",
        "#     if relevant_messages:\n",
        "#         combined_scenario = \" \".join(relevant_messages)\n",
        "#         # If too long, use only the most substantial messages\n",
        "#         if len(combined_scenario) > 500:\n",
        "#             # Find the longest message, which likely contains the main scenario\n",
        "#             longest_message = max(relevant_messages, key=len)\n",
        "#             return longest_message\n",
        "#         return combined_scenario\n",
        "\n",
        "#     return \"generic dataset of various objects and scenes\"\n",
        "\n",
        "\n",
        "def get_current_user_question(state):\n",
        "    \"\"\"Extract the most recent user question from the state\"\"\"\n",
        "    for message in reversed(state[\"messages\"]):\n",
        "        if isinstance(message, HumanMessage):\n",
        "            return message.content\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def parse_image_prompts(response_text):\n",
        "    \"\"\"Extract a list of prompts from the LLM response\"\"\"\n",
        "    try:\n",
        "        # Try to find a Python list in the text\n",
        "        match = re.search(r'\\[(.*?)\\]', response_text, re.DOTALL)\n",
        "        if match:\n",
        "            prompts_text = match.group(1)\n",
        "            # Safely evaluate the list\n",
        "            return eval(f\"[{prompts_text}]\")\n",
        "\n",
        "        # If no list is found, try to extract numbered items\n",
        "        lines = response_text.split('\\n')\n",
        "        prompts = []\n",
        "        for line in lines:\n",
        "            # Look for numbered lines like \"1. prompt\" or \"1) prompt\"\n",
        "            match = re.match(r'^\\s*\\d+[\\.\\)]\\s*(.*)', line)\n",
        "            if match:\n",
        "                prompts.append(match.group(1).strip())\n",
        "\n",
        "        if prompts:\n",
        "            return prompts\n",
        "\n",
        "        # If all else fails, split by newlines and filter\n",
        "        return [line.strip() for line in lines if line.strip() and len(line.strip()) > 10]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing prompts: {e}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "ZH4SpFOpufUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of using extract_scenario - Use a llm based scenario updater to update scenario in runtime based on the conversation history.\n",
        "\n",
        "def updated_scenario(state):\n",
        "  scenario = state['scenario']\n",
        "  scenario_update_llm = ChatGroq(groq_api_key=groq_api_key, model=\"llama-3.1-8b-instant\")\n",
        "\n",
        "  system = f\"\"\" You have a scenario which will be used to generate diverse prompts each will be used for\n",
        "  for image generation. What your task is I will provide you initial scenario and you have to update it\n",
        "  accordingly to the user instructions. You will have both user instructions and the scenario, return the updated scenario please.\n",
        "  Scenario: {scenario}\n",
        "  Instruction from User: {state['messages'][-1]}\n",
        "  \"\"\"\n",
        "\n",
        "  # Update the prompt based on the given user instruction\n",
        "  scenario_update_prompt = ChatPromptTemplate.from_messages(\n",
        "      [\n",
        "          (\"system\", system),\n",
        "          (\"human\", \"{question}\"),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  scenario_update_agent = scenario_update_prompt | scenario_update_llm\n",
        "  response = scenario_update_agent.invoke(state['messages'][-1])\n",
        "\n",
        "  #state['scenario'] = response.content\n",
        "  return response.content\n"
      ],
      "metadata": {
        "id": "vDECCH6ZufSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Router"
      ],
      "metadata": {
        "id": "gro1TnNG6oBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To create the actuall router chain which will be used to route questions\n",
        "\n",
        "def create_router(router_agent):\n",
        "    \"\"\"Create an LLM router to determine which agent should handle the request\"\"\"\n",
        "\n",
        "    # Use JsonOutputParser to format LLM output\n",
        "    parser = JsonOutputParser(pydantic_object=RouterOutput)\n",
        "\n",
        "    system = \"\"\"You are an expert routing agent for an image dataset creation system.\n",
        "    Your job is to analyze the user's message and determine which of the following agents should handle it:\n",
        "\n",
        "    1. conversation_agent: For general conversation, questions about the system, or when the user is still describing their needs.\n",
        "\n",
        "    2. sample_image_agent: When the user is explicitly or implicitly requesting sample images, wants to see examples,\n",
        "       or is describing specific image requirements that would benefit from visual confirmation before proceeding.\n",
        "\n",
        "    3. dataset_generator_agent: When the user is ready to generate the full dataset of images, expressing satisfaction with samples,\n",
        "       or directly asking to proceed with dataset creation.\n",
        "\n",
        "    Analyze the content, context, and intent of the user's message, then select the most appropriate agent.\n",
        "\n",
        "    Output your decision as a JSON object with the following fields:\n",
        "    - agent: The name of the agent (one of the three above)\n",
        "    - confidence: Your confidence level in this decision (0.0 to 1.0)\n",
        "    - reasoning: A brief explanation of why you chose this agent\n",
        "    \"\"\"\n",
        "\n",
        "    router_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"User message: {question}\\n\\nConversation history summary: {history_summary}\")\n",
        "    ])\n",
        "\n",
        "    router_chain = router_prompt | router_agent | parser\n",
        "\n",
        "    return router_chain\n"
      ],
      "metadata": {
        "id": "rWeDiBH24AQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a summary of conversation history\n",
        "def summarize_conversation(state):\n",
        "    \"\"\"Create a brief summary of the conversation history for context\"\"\"\n",
        "    history = []\n",
        "\n",
        "    # Extract last few turns of conversation\n",
        "    messages = state[\"messages\"][-10:]  # Limit to last 10 messages\n",
        "\n",
        "    for msg in messages:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            history.append(f\"User: {msg.content}\")\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            # Truncate very long assistant responses\n",
        "            content = msg.content\n",
        "            if len(content) > 200:\n",
        "                content = content[:197] + \"...\"\n",
        "            history.append(f\"Assistant: {content}\")\n",
        "\n",
        "    return \"\\n\".join(history)\n",
        "\n",
        "\n",
        "# The actual Routing function\n",
        "def route_question(state: GraphState, router_chain):\n",
        "    \"\"\"Route the user question to the appropriate agent\"\"\"\n",
        "\n",
        "    question = get_current_user_question(state)\n",
        "    history_summary = summarize_conversation(state)\n",
        "\n",
        "    # Make routing decision\n",
        "    try:\n",
        "        route_decision = router_chain.invoke({\n",
        "            \"question\": question,\n",
        "            \"history_summary\": history_summary\n",
        "        })\n",
        "\n",
        "        print(f\"Router Decision: {route_decision}\")\n",
        "\n",
        "        # Update state to track which agent was selected\n",
        "        state[\"current_agent\"] = route_decision.agent\n",
        "\n",
        "        # Set dataset_ready flag if we're routing to dataset generator\n",
        "        if route_decision.agent == \"dataset_generator_agent\":\n",
        "            state[\"dataset_ready\"] = True\n",
        "\n",
        "        # Map agent names to graph nodes\n",
        "        agent_to_node = {\n",
        "            \"conversation_agent\": \"conversation\",\n",
        "            \"sample_image_agent\": \"generate_sample_images\",\n",
        "            \"dataset_generator_agent\": \"generate_prompts\"\n",
        "        }\n",
        "\n",
        "        return agent_to_node[route_decision.agent]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Routing error: {e}\")\n",
        "        # Default to conversation as fallback\n",
        "        return \"conversation\""
      ],
      "metadata": {
        "id": "AuT3HvqIufN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent Handlers"
      ],
      "metadata": {
        "id": "pjHFigtt8syV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conversation_handler(state: GraphState):\n",
        "    \"\"\"The primary conversation agent to interact with the user\"\"\"\n",
        "\n",
        "    user_message = state[\"messages\"][-1].content\n",
        "\n",
        "    system_message = \"\"\"You are an AI assistant specializing in helping users create image datasets.\n",
        "    Your role is to understand what kind of image dataset the user wants to create.\n",
        "\n",
        "    Be conversational and helpful, focusing on understanding the user's requirements clearly.\n",
        "    If the user seems to be describing a specific type of images, suggest they might want to see samples.\n",
        "    If they've seen samples and like them, you can suggest generating the full dataset.\n",
        "\n",
        "    Keep your responses concise and focused on helping create their image dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_message),\n",
        "        (\"human\", \"{user_input}\")\n",
        "    ])\n",
        "\n",
        "    conversation_agent = state[\"conversation_agent\"]\n",
        "    chain = prompt | conversation_agent\n",
        "    response = chain.invoke({\"user_input\": user_message})\n",
        "\n",
        "    # Add to conversation history\n",
        "    conversation_id = len(state[\"conversation_history\"]) + 1\n",
        "    state[\"conversation_history\"][conversation_id] = {\n",
        "        \"question\": user_message,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    state[\"messages\"].append(AIMessage(content=response.content))\n",
        "    state[\"current_agent\"] = \"conversation_agent\"\n",
        "    state['scenario'] = updated_scenario(state)\n",
        "\n",
        "    return \"router\"\n",
        "\n",
        "\n",
        "def sample_image_handler(state: GraphState):\n",
        "    \"\"\"Generate sample images for user review\"\"\"\n",
        "\n",
        "    # Extract user scenario from conversation\n",
        "    user_scenario = state['scenario']\n",
        "\n",
        "    # Generate a few sample prompts\n",
        "    system = \"\"\"You are a prompt generator for text-to-image models.\n",
        "    Generate 3 diverse and detailed prompts for sample images based on the user scenario.\n",
        "    Each prompt should be descriptive and optimized for image generation.\n",
        "    Format your output as a Python list of exactly 3 strings: [\"prompt1\", \"prompt2\", \"prompt3\"]\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system),\n",
        "        (\"human\", f\"Generate 3 sample image prompts for scenario: {user_scenario}\")\n",
        "    ])\n",
        "\n",
        "    prompt_generator = state[\"prompt_generator\"]\n",
        "    chain = prompt | prompt_generator\n",
        "    response = chain.invoke({})\n",
        "\n",
        "    # Extract the prompt list\n",
        "    sample_prompts = parse_image_prompts(response.content)\n",
        "\n",
        "    # Ensure we have exactly 3 prompts\n",
        "    if len(sample_prompts) < 3:\n",
        "        # Fill in missing prompts\n",
        "        while len(sample_prompts) < 3:\n",
        "            sample_prompts.append(f\"{user_scenario} variation {len(sample_prompts)+1}\")\n",
        "    elif len(sample_prompts) > 3:\n",
        "        # Trim to just 3 prompts\n",
        "        sample_prompts = sample_prompts[:3]\n",
        "\n",
        "    # Generate sample images\n",
        "    sample_image_urls = []\n",
        "    for prompt in sample_prompts:\n",
        "        try:\n",
        "            response = state[\"image_client\"].images.generate(\n",
        "                model=\"dall-e-2\",\n",
        "                prompt=prompt,\n",
        "                size=\"256x256\",\n",
        "                quality=\"standard\",\n",
        "                n=1\n",
        "            )\n",
        "            sample_image_urls.append(response.data[0].url)\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating image: {str(e)}\")\n",
        "            sample_image_urls.append(None)\n",
        "\n",
        "    # Filter out any None values\n",
        "    sample_image_urls = [url for url in sample_image_urls if url is not None]\n",
        "\n",
        "    state[\"sample_images\"] = sample_image_urls\n",
        "    state[\"prompt_list\"] = sample_prompts\n",
        "\n",
        "    # Create response message with sample images\n",
        "    response_text = \"Here are some sample images based on your requirements:\\n\\n\"\n",
        "    for i, (prompt, url) in enumerate(zip(sample_prompts, sample_image_urls)):\n",
        "        if url:\n",
        "            response_text += f\"Sample {i+1}:\\nPrompt: {prompt}\\n\\n\"\n",
        "\n",
        "    response_text += \"How do these look? Would you like to make any adjustments before generating the full dataset of 50 images?\"\n",
        "\n",
        "    state[\"messages\"].append(AIMessage(content=response_text))\n",
        "    state[\"current_agent\"] = \"sample_image_agent\"\n",
        "\n",
        "    return \"router\"\n",
        "\n"
      ],
      "metadata": {
        "id": "JqkWCfGD8u9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_generator_handler(state: GraphState):\n",
        "    \"\"\"Generate the full set of prompts based on user requirements\"\"\"\n",
        "\n",
        "    # Extract user scenario from conversation\n",
        "    user_scenario = updated_state(state)\n",
        "    user_feedback = state.get(\"user_feedback\", \"\")\n",
        "\n",
        "    # Include user feedback if available\n",
        "    scenario_with_feedback = user_scenario\n",
        "    if user_feedback:\n",
        "        scenario_with_feedback = f\"{user_scenario}. User feedback on samples: {user_feedback}\"\n",
        "\n",
        "    system = \"\"\"You are a prompt generator for text to image models. Your task is to generate 50 diverse prompts\n",
        "    for text-to-image models to create a comprehensive dataset given the scenario.\n",
        "    Each prompt should be descriptive and optimized for image generation.\n",
        "    Make sure to cover different angles, perspectives, environments, and contexts.\n",
        "    Format your output as a Python list of strings: [\"prompt1\", \"prompt2\", ...]\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system),\n",
        "        (\"human\", f\"Generate 50 diverse image prompts for scenario: {scenario_with_feedback}\")\n",
        "    ])\n",
        "\n",
        "    prompt_generator = state[\"prompt_generator\"]\n",
        "    chain = prompt | prompt_generator\n",
        "    response = chain.invoke({})\n",
        "\n",
        "    # Extract the prompt list\n",
        "    prompt_list = parse_image_prompts(response.content)\n",
        "\n",
        "    # Ensure we have at least 50 prompts\n",
        "    if len(prompt_list) < 50:\n",
        "        # Fill in missing prompts\n",
        "        while len(prompt_list) < 50:\n",
        "            prompt_list.append(f\"{user_scenario} variation {len(prompt_list)+1}\")\n",
        "\n",
        "    state[\"prompt_list\"] = prompt_list[:50]  # Limit to exactly 50 prompts\n",
        "    state[\"dataset_ready\"] = True\n",
        "\n",
        "    # Prepare response message\n",
        "    response_text = f\"I've generated 50 diverse prompts for your dataset based on your requirements.\\n\\n\"\n",
        "    response_text += \"Here are a few examples:\\n\"\n",
        "    for i, prompt in enumerate(prompt_list[:5]):\n",
        "        response_text += f\"{i+1}. {prompt}\\n\"\n",
        "\n",
        "    response_text += f\"\\n...and {len(prompt_list)-5} more prompts.\\n\\n\"\n",
        "    response_text += \"Would you like me to proceed with generating all 50 images for your dataset now?\"\n",
        "\n",
        "    state[\"messages\"].append(AIMessage(content=response_text))\n",
        "    state[\"current_agent\"] = \"dataset_generator_agent\"\n",
        "\n",
        "    return \"router\"\n",
        "\n",
        "\n",
        "def dataset_generator_handler(state: GraphState):\n",
        "    \"\"\"Generate the full dataset of images\"\"\"\n",
        "\n",
        "    prompt_list = state[\"prompt_list\"][:50]  # Ensure we use max 50 prompts\n",
        "    image_urls = []\n",
        "\n",
        "    # Create a progress update message\n",
        "    progress_message = \"Starting to generate your dataset with 50 images. This might take some time...\\n\"\n",
        "    state[\"messages\"].append(AIMessage(content=progress_message))\n",
        "\n",
        "    # Generate images for each prompt - for real implementation\n",
        "    for idx, prompt in enumerate(prompt_list):\n",
        "        try:\n",
        "            print(f\"[{idx+1}/{len(prompt_list)}] Generating image for prompt: {prompt}\")\n",
        "\n",
        "            response = state[\"image_client\"].images.generate(\n",
        "                model=\"dall-e-2\",\n",
        "                prompt=prompt,\n",
        "                size=\"256x256\",\n",
        "                quality=\"standard\",\n",
        "                n=1\n",
        "            )\n",
        "\n",
        "            image_url = response.data[0].url\n",
        "            image_urls.append(image_url)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error generating image {idx+1}: {str(e)}\"\n",
        "            print(error_msg)\n",
        "            image_urls.append(None)\n",
        "\n",
        "    # Filter out None values\n",
        "    image_urls = [url for url in image_urls if url is not None]\n",
        "\n",
        "    # Final response with dataset information\n",
        "    final_message = f\"âœ… Dataset generation complete! Generated {len(image_urls)}/50 images.\\n\\n\"\n",
        "\n",
        "    # Include some sample URLs\n",
        "    if image_urls:\n",
        "        final_message += \"Here are a few sample images from your dataset:\\n\"\n",
        "        for i, url in enumerate(image_urls[:5]):\n",
        "            if url:\n",
        "                final_message += f\"Image {i+1}: {url}\\n\"\n",
        "\n",
        "    final_message += \"\\nYou can download these images to assemble your complete dataset. Would you like me to help with anything else?\"\n",
        "\n",
        "    state[\"messages\"].append(AIMessage(content=final_message))\n",
        "    state[\"current_agent\"] = \"dataset_generator_agent\"\n",
        "\n",
        "    return \"router\"\n"
      ],
      "metadata": {
        "id": "lrYZ4jDs8vpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Router Handler"
      ],
      "metadata": {
        "id": "bIiku9D2Dy9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Router handler function\n",
        "def router_handler(state: GraphState):\n",
        "    \"\"\"Route the conversation to the appropriate handler based on LLM decision\"\"\"\n",
        "    router_chain = state[\"router_chain\"]\n",
        "    return route_question(state, router_chain)"
      ],
      "metadata": {
        "id": "R-MeE14JDvRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langgraph application"
      ],
      "metadata": {
        "id": "T2tRKSxaDrkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the LangGraph application\n",
        "def create_image_dataset_graph(agents):\n",
        "    conversation_agent, prompt_generator, image_client, router_agent = agents\n",
        "\n",
        "    # Create the router chain\n",
        "    router_chain = create_router(router_agent)\n",
        "\n",
        "    # Create the graph\n",
        "    workflow = StateGraph(GraphState)\n",
        "\n",
        "    # Add nodes\n",
        "    workflow.add_node(\"conversation\", conversation_handler)\n",
        "    workflow.add_node(\"generate_sample_images\", sample_image_handler)\n",
        "    workflow.add_node(\"generate_prompts\", prompt_generator_handler)\n",
        "    workflow.add_node(\"generate_dataset\", dataset_generator_handler)\n",
        "    workflow.add_node(\"router\", router_handler)\n",
        "\n",
        "    # Add edges from router to specific agents\n",
        "    workflow.add_edge(\"router\", \"conversation\")\n",
        "    workflow.add_edge(\"router\", \"generate_sample_images\")\n",
        "    workflow.add_edge(\"router\", \"generate_prompts\")\n",
        "\n",
        "    # Add edges from agents back to router\n",
        "    workflow.add_edge(\"conversation\", \"router\")\n",
        "    workflow.add_edge(\"generate_sample_images\", \"router\")\n",
        "    workflow.add_edge(\"generate_prompts\", \"router\")\n",
        "\n",
        "    # Add conditional edge from router to dataset generator\n",
        "    workflow.add_conditional_edges(\n",
        "        \"router\",\n",
        "        lambda state: state.get(\"dataset_ready\", False),\n",
        "        {\n",
        "            True: \"generate_dataset\",\n",
        "            False: \"router\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Dataset generator goes back to router when done\n",
        "    workflow.add_edge(\"generate_dataset\", \"router\")\n",
        "\n",
        "    # Compile the graph\n",
        "    app = workflow.compile()\n",
        "\n",
        "    return app, router_chain\n"
      ],
      "metadata": {
        "id": "P4VdqnnjDq3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Session Management functions"
      ],
      "metadata": {
        "id": "mquGRvyLDl0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_new_session(session_id, agents, router_chain):\n",
        "    \"\"\"Create a new conversation session\"\"\"\n",
        "    conversation_agent, prompt_generator, image_client, router_agent = agents\n",
        "\n",
        "    # Initialize the state\n",
        "    initial_state = {\n",
        "        \"messages\": [AIMessage(content=\"Hello! I'm your image dataset creation assistant. What kind of image dataset would you like to create today?\")],\n",
        "        \"conversation_history\": {},\n",
        "        \"current_agent\": \"conversation_agent\",\n",
        "        \"prompt_list\": [],\n",
        "        \"sample_images\": [],\n",
        "        \"user_feedback\": \"\",\n",
        "        \"dataset_ready\": False,\n",
        "        \"llm_model\": \"Gemma2-9b-It\",\n",
        "        \"conversation_agent\": conversation_agent,\n",
        "        \"prompt_generator\": prompt_generator,\n",
        "        \"image_client\": image_client,\n",
        "        \"router_agent\": router_agent,\n",
        "        \"router_chain\": router_chain,\n",
        "        \"session_id\": session_id,\n",
        "        \"scenario\": \"\"\n",
        "    }\n",
        "    return initial_state\n",
        "\n"
      ],
      "metadata": {
        "id": "kCd3A71v8vds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "API Functions"
      ],
      "metadata": {
        "id": "3Ix_E_YWX9Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    # Load environment variables - in production use proper env management\n",
        "    groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "    openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not groq_api_key or not openai_api_key:\n",
        "        raise ValueError(\"API keys not found in environment variables\")\n",
        "\n",
        "    # Initialize agents\n",
        "    app.state.agents = initialize_agents(groq_api_key, openai_api_key)\n",
        "    app.state.graph, app.state.router_chain = create_image_dataset_graph(app.state.agents)\n",
        "\n",
        "    yield"
      ],
      "metadata": {
        "id": "zA4UpXAn8vaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize FastAPI app\n",
        "app = FastAPI(lifespan=lifespan)\n",
        "\n",
        "# Add CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Adjust this in production\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")"
      ],
      "metadata": {
        "id": "q8cfvLoMufLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models for API\n",
        "class UserMessage(BaseModel):\n",
        "    message: str\n",
        "    session_id: str\n",
        "\n",
        "class AIResponse(BaseModel):\n",
        "    response: str\n",
        "    has_images: bool = False\n",
        "    image_urls: List[str] = []\n",
        "    session_id: str\n",
        "\n",
        "class RouterOutput(BaseModel):\n",
        "    \"\"\"Output schema for the router agent\"\"\"\n",
        "    agent: Literal[\"conversation_agent\", \"sample_image_agent\", \"dataset_generator_agent\"]\n",
        "    confidence: float = Field(description=\"Confidence level from 0.0 to 1.0\")\n",
        "    reasoning: str = Field(description=\"Brief explanation of why this agent was chosen\")\n"
      ],
      "metadata": {
        "id": "e7mEITgSpuj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API endpoints\n",
        "@app.post(\"/chat\", response_model=AIResponse)\n",
        "async def chat(user_message: UserMessage):\n",
        "    \"\"\"Process a user message using the LangGraph workflow\"\"\"\n",
        "    session_id = user_message.session_id\n",
        "    message = user_message.message\n",
        "\n",
        "    # Get or create session\n",
        "    if session_id not in active_sessions:\n",
        "        active_sessions[session_id] = create_new_session(session_id, app.state.agents, app.state.router_chain)\n",
        "\n",
        "    state = active_sessions[session_id]\n",
        "\n",
        "    # Add user message to state\n",
        "    state[\"messages\"].append(HumanMessage(content=message))\n",
        "\n",
        "    # Check for any feedback to sample images\n",
        "    if state[\"current_agent\"] == \"sample_image_agent\" and message:\n",
        "        state[\"user_feedback\"] = message\n",
        "\n",
        "    # Process message through the graph - always start at router\n",
        "    new_state = app.state.graph.invoke(state, {\"starting_node\": \"router\"})\n",
        "    active_sessions[session_id] = new_state\n",
        "\n",
        "    # Extract response for the API\n",
        "    ai_response = new_state[\"messages\"][-1].content\n",
        "\n",
        "    # Check if we have sample images to return\n",
        "    has_images = False\n",
        "    image_urls = []\n",
        "\n",
        "    if \"sample_images\" in new_state and new_state[\"sample_images\"]:\n",
        "        has_images = True\n",
        "        image_urls = new_state[\"sample_images\"]\n",
        "\n",
        "    # If we just completed dataset generation, include those images\n",
        "    if new_state[\"current_agent\"] == \"dataset_generator_agent\" and \"generate_dataset\" in str(new_state):\n",
        "        has_images = True\n",
        "        # Get the full list of generated images if available\n",
        "        # This would need to be implemented based on how you store the generated images\n",
        "\n",
        "    return AIResponse(\n",
        "        response=ai_response,\n",
        "        has_images=has_images,\n",
        "        image_urls=image_urls,\n",
        "        session_id=session_id\n",
        "    )\n"
      ],
      "metadata": {
        "id": "q6Wvg-LtoC7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.get(\"/sessions/{session_id}\")\n",
        "async def get_session(session_id: str):\n",
        "    \"\"\"Get information about a specific session\"\"\"\n",
        "    if session_id not in active_sessions:\n",
        "        raise HTTPException(status_code=404, detail=\"Session not found\")\n",
        "\n",
        "    state = active_sessions[session_id]\n",
        "\n",
        "    # Extract conversation messages for the frontend\n",
        "    messages = []\n",
        "    for msg in state[\"messages\"]:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            messages.append({\"role\": \"user\", \"content\": msg.content})\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "\n",
        "    return {\n",
        "        \"session_id\": session_id,\n",
        "        \"messages\": messages,\n",
        "        \"current_agent\": state[\"current_agent\"],\n",
        "        \"has_samples\": len(state.get(\"sample_images\", [])) > 0\n",
        "    }"
      ],
      "metadata": {
        "id": "sm9rmWdloC3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.post(\"/sessions/new\")\n",
        "async def create_session():\n",
        "    \"\"\"Create a new session\"\"\"\n",
        "    import uuid\n",
        "\n",
        "    session_id = str(uuid.uuid4())\n",
        "    active_sessions[session_id] = create_new_session(session_id, app.state.agents, app.state.router_chain)\n",
        "\n",
        "    return {\"session_id\": session_id}\n",
        "\n",
        "\n",
        "@app.delete(\"/sessions/{session_id}\")\n",
        "async def delete_session(session_id: str):\n",
        "    \"\"\"Delete a session\"\"\"\n",
        "    if session_id in active_sessions:\n",
        "        del active_sessions[session_id]\n",
        "\n",
        "    return {\"status\": \"deleted\", \"session_id\": session_id}\n",
        "\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    # Load environment variables - in production use proper env management\n",
        "    groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "    openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not groq_api_key or not openai_api_key:\n",
        "        raise RuntimeError(\"API keys not found in environment variables\")\n",
        "\n",
        "    # Initialize agents\n",
        "    app.state.agents = initialize_agents(groq_api_key, openai_api_key)\n",
        "    app.state.graph, app.state.router_chain = create_image_dataset_graph(app.state.agents)\n",
        "\n",
        "    yield\n"
      ],
      "metadata": {
        "id": "Y0t6KeZtoXTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "id": "4KEZJapVoWrC",
        "outputId": "4c76c8e3-dd45-4798-8c1d-ec3e1e8404b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [635]\n",
            "INFO:     Waiting for application startup.\n",
            "ERROR:    Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 692, in lifespan\n",
            "    async with self.lifespan_context(app) as maybe_state:\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 210, in __aenter__\n",
            "    return await anext(self.gen)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-6117aa2d9826>\", line 8, in lifespan\n",
            "    raise ValueError(\"API keys not found in environment variables\")\n",
            "ValueError: API keys not found in environment variables\n",
            "\n",
            "ERROR:    Application startup failed. Exiting.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "3",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph langsmith langchain langchain_groq langchain_community langchain_openai FastAPI uvicorn nest_asyncio"
      ],
      "metadata": {
        "id": "bOHkuVv4oWlA",
        "collapsed": true,
        "outputId": "6c2eec6f-aa0d-4d9f-8e9a-becea1b25b50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.29-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting FastAPI\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.51)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.61-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.16)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.11.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.22.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.70.0)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from FastAPI)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from FastAPI) (4.13.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langgraph-0.3.29-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading groq-0.22.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.61-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: xxhash, uvicorn, python-dotenv, ormsgpack, mypy-extensions, marshmallow, httpx-sse, typing-inspect, tiktoken, starlette, pydantic-settings, langgraph-sdk, groq, FastAPI, dataclasses-json, langgraph-checkpoint, langchain_openai, langchain_groq, langgraph-prebuilt, langgraph, langchain_community\n",
            "Successfully installed FastAPI-0.115.12 dataclasses-json-0.6.7 groq-0.22.0 httpx-sse-0.4.0 langchain_community-0.3.21 langchain_groq-0.3.2 langchain_openai-0.3.12 langgraph-0.3.29 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.61 marshmallow-3.26.1 mypy-extensions-1.0.0 ormsgpack-1.9.1 pydantic-settings-2.8.1 python-dotenv-1.1.0 starlette-0.46.1 tiktoken-0.9.0 typing-inspect-0.9.0 uvicorn-0.34.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Dict, List, Any, Optional, Literal\n",
        "from fastapi import FastAPI, HTTPException, Body, Depends\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_openai import OpenAI\n",
        "import openai\n",
        "from langgraph.graph import StateGraph, END\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from contextlib import asynccontextmanager\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "print(\"The app is runing\")\n",
        "\n",
        "# FastAPI app creation with startup and shutdown events\n",
        "#\"\" @asynccontextmanager\n",
        "# async def lifespan(app: FastAPI):\n",
        "#     # Load environment variables - in production use proper env management\n",
        "#     global default_groq_api_key, default_openai_api_key\n",
        "\n",
        "#     load_dotenv()\n",
        "\n",
        "#     default_groq_api_key = os.getenv(\"GROQ_API_KEY\", \"\")\n",
        "#     default_openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "\n",
        "#     if not default_groq_api_key or not default_openai_api_key:\n",
        "#         print(\"Warning: API keys not found in environment variables. Users will need to provide them.\")\n",
        "\n",
        "#     # Initialize agents with default keys if available\n",
        "#     if default_groq_api_key and default_openai_api_key:\n",
        "#         app.state.agents = initialize_agents(default_groq_api_key, default_openai_api_key)\n",
        "#         app.state.graph, app.state.router_chain = create_image_dataset_graph(app.state.agents)\n",
        "#     else:\n",
        "#         app.state.agents = None\n",
        "#         app.state.graph = None\n",
        "#         app.state.router_chain = None\n",
        "\n",
        "#     yield\n",
        "\n",
        "    # Cleanup code here if needed\n",
        "\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Add CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Adjust this in production\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "\n",
        "# Define models for API\n",
        "class UserMessage(BaseModel):\n",
        "    message: str\n",
        "    session_id: str\n",
        "\n",
        "# API endpoints\n",
        "@app.post(\"/chat\", response_model=AIResponse)\n",
        "async def chat(user_message: UserMessage):\n",
        "    \"\"\"Process a user message using the LangGraph workflow\"\"\"\n",
        "    session_id = user_message.session_id\n",
        "    message = user_message.message\n",
        "\n",
        "    # Get or create session\n",
        "    if session_id not in active_sessions:\n",
        "        # Check if default agents are available\n",
        "        if app.state.agents is None:\n",
        "            raise HTTPException(\n",
        "                status_code=400,\n",
        "                detail=\"No API keys set. Please set API keys using the /set_api_keys endpoint first.\"\n",
        "            )\n",
        "        active_sessions[session_id] = create_new_session(session_id, app.state.agents, app.state.router_chain)\n",
        "\n",
        "    state = active_sessions[session_id]\n",
        "\n",
        "    # Add user message to state\n",
        "    state[\"messages\"].append(HumanMessage(content=message))\n",
        "\n",
        "    # Check for any feedback to sample images\n",
        "    if state[\"current_agent\"] == \"sample_image_agent\" and message:\n",
        "        state[\"user_feedback\"] = message\n",
        "\n",
        "    # Process message through the graph - always start at router\n",
        "    new_state = app.state.graph.invoke(state, {\"starting_node\": \"router\"})\n",
        "    active_sessions[session_id] = new_state\n",
        "\n",
        "    # Extract response for the API\n",
        "    ai_response = new_state[\"messages\"][-1].content\n",
        "\n",
        "    # Check if we have sample images to return\n",
        "    has_images = False\n",
        "    image_urls = []\n",
        "\n",
        "    if \"sample_images\" in new_state and new_state[\"sample_images\"]:\n",
        "        has_images = True\n",
        "        image_urls = new_state[\"sample_images\"]\n",
        "\n",
        "    # If we just completed dataset generation, include those images\n",
        "    if new_state[\"current_agent\"] == \"dataset_generator_agent\" and \"generate_dataset\" in str(new_state):\n",
        "        has_images = True\n",
        "        # Get the full list of generated images if available\n",
        "\n",
        "    return AIResponse(\n",
        "        response=ai_response,\n",
        "        has_images=has_images,\n",
        "        image_urls=image_urls,\n",
        "        session_id=session_id\n",
        "    )\n",
        "\n",
        "\n",
        "class DatasetParameters(BaseModel):\n",
        "    \"\"\"Parameters for dataset generation\"\"\"\n",
        "    num_images: int = Field(default=50, ge=5, le=200, description=\"Number of images to generate (5-200)\")\n",
        "    resolution: str = Field(default=\"256x256\", description=\"Image resolution\")\n",
        "    session_id: str = Field(description=\"Session ID to apply these parameters to\")\n",
        "\n",
        "\n",
        "@app.post(\"/set_dataset_parameters\")\n",
        "async def set_dataset_parameters(params: DatasetParameters):\n",
        "    \"\"\"Set custom parameters for dataset generation\"\"\"\n",
        "    session_id = params.session_id\n",
        "\n",
        "    # Check if session exists\n",
        "    if session_id not in active_sessions:\n",
        "        raise HTTPException(status_code=404, detail=\"Session not found\")\n",
        "\n",
        "    # Update session state with new parameters\n",
        "    active_sessions[session_id][\"num_images\"] = params.num_images\n",
        "    active_sessions[session_id][\"image_resolution\"] = params.resolution\n",
        "\n",
        "    # Inform user about the parameter change\n",
        "    info_message = f\"Dataset parameters updated: {params.num_images} images at {params.resolution} resolution.\"\n",
        "    active_sessions[session_id][\"messages\"].append(AIMessage(content=info_message))\n",
        "\n",
        "    return {\n",
        "        \"status\": \"success\",\n",
        "        \"message\": f\"Dataset parameters updated: {params.num_images} images at {params.resolution} resolution\",\n",
        "        \"session_id\": session_id\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ApiKeys(BaseModel):\n",
        "    \"\"\"API keys for external services\"\"\"\n",
        "    groq_api_key: str = Field(description=\"API key for Groq\")\n",
        "    openai_api_key: str = Field(description=\"API key for OpenAI\")\n",
        "    session_id: str = Field(description=\"Session ID to apply these keys to\")\n",
        "\n",
        "\n",
        "@app.post(\"/set_api_keys\")\n",
        "async def set_api_keys(keys: ApiKeys):\n",
        "    \"\"\"Set custom API keys for external services\"\"\"\n",
        "    session_id = keys.session_id\n",
        "\n",
        "    # Check if session exists, create if not\n",
        "    if session_id not in active_sessions:\n",
        "        # Initialize agents with the provided keys\n",
        "        try:\n",
        "            agents = initialize_agents(keys.groq_api_key, keys.openai_api_key)\n",
        "            graph, router_chain = create_image_dataset_graph(agents)\n",
        "\n",
        "            # Store in app state for this session only\n",
        "            if not hasattr(app.state, 'custom_session_agents'):\n",
        "                app.state.custom_session_agents = {}\n",
        "\n",
        "            app.state.custom_session_agents[session_id] = {\n",
        "                'agents': agents,\n",
        "                'graph': graph,\n",
        "                'router_chain': router_chain\n",
        "            }\n",
        "\n",
        "            # Create new session with these agents\n",
        "            active_sessions[session_id] = create_new_session(\n",
        "                session_id,\n",
        "                agents,\n",
        "                router_chain\n",
        "            )\n",
        "\n",
        "            # Update API keys in state\n",
        "            active_sessions[session_id][\"groq_api_key\"] = keys.groq_api_key\n",
        "            active_sessions[session_id][\"openai_api_key\"] = keys.openai_api_key\n",
        "\n",
        "            return {\n",
        "                \"status\": \"success\",\n",
        "                \"message\": \"API keys set and new session created\",\n",
        "                \"session_id\": session_id\n",
        "            }\n",
        "        except Exception as e:\n",
        "            raise HTTPException(\n",
        "                status_code=400,\n",
        "                detail=f\"Failed to initialize agents with provided keys: {str(e)}\"\n",
        "            )\n",
        "    else:\n",
        "        # Update existing session with new keys\n",
        "        success = update_session_agents(session_id, keys.groq_api_key, keys.openai_api_key)\n",
        "\n",
        "        if success:\n",
        "            return {\n",
        "                \"status\": \"success\",\n",
        "                \"message\": \"API keys updated for existing session\",\n",
        "                \"session_id\": session_id\n",
        "            }\n",
        "        else:\n",
        "            raise HTTPException(\n",
        "                status_code=400,\n",
        "                detail=\"Failed to update session with new API keys\"\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@app.get(\"/sessions/{session_id}\")\n",
        "async def get_session(session_id: str):\n",
        "    \"\"\"Get information about a specific session\"\"\"\n",
        "    if session_id not in active_sessions:\n",
        "        raise HTTPException(status_code=404, detail=\"Session not found\")\n",
        "\n",
        "    state = active_sessions[session_id]\n",
        "\n",
        "    # Extract conversation messages for the frontend\n",
        "    messages = []\n",
        "    for msg in state[\"messages\"]:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            messages.append({\"role\": \"user\", \"content\": msg.content})\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "\n",
        "    return {\n",
        "        \"session_id\": session_id,\n",
        "        \"messages\": messages,\n",
        "        \"current_agent\": state[\"current_agent\"],\n",
        "        \"has_samples\": len(state.get(\"sample_images\", [])) > 0,\n",
        "        \"num_images\": state[\"num_images\"],\n",
        "        \"image_resolution\": state[\"image_resolution\"],\n",
        "        \"dataset_ready\": state[\"dataset_ready\"]\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "@app.post(\"/sessions/new\")\n",
        "async def create_session():\n",
        "    \"\"\"Create a new session\"\"\"\n",
        "    import uuid\n",
        "\n",
        "    session_id = str(uuid.uuid4())\n",
        "    print(f\"Creating new session with ID: {session_id}\")\n",
        "\n",
        "    # Check if default agents are available\n",
        "    if app.state.agents is None:\n",
        "        print(\"Error: Agents not initialized. API keys are required.\")\n",
        "        return {\n",
        "            \"session_id\": session_id,\n",
        "            \"status\": \"keys_required\",\n",
        "            \"message\": \"Session created but API keys are required before use. Please call /set_api_keys endpoint.\"\n",
        "        }\n",
        "\n",
        "    active_sessions[session_id] = create_new_session(session_id, app.state.agents, app.state.router_chain)\n",
        "    print(f\"Session {session_id} created successfully.\")\n",
        "\n",
        "    return {\n",
        "        \"session_id\": session_id,\n",
        "        \"status\": \"success\",\n",
        "        \"message\": \"New session created successfully\"\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "@app.delete(\"/sessions/{session_id}\")\n",
        "async def delete_session(session_id: str):\n",
        "    \"\"\"Delete a session\"\"\"\n",
        "    if session_id not in active_sessions:\n",
        "        raise HTTPException(status_code=404, detail=\"Session not found\")\n",
        "\n",
        "    # Remove the session\n",
        "    del active_sessions[session_id]\n",
        "\n",
        "    # Also clean up any custom agents if they exist\n",
        "    if hasattr(app.state, 'custom_session_agents') and session_id in app.state.custom_session_agents:\n",
        "        del app.state.custom_session_agents[session_id]\n",
        "\n",
        "    return {\n",
        "        \"status\": \"success\",\n",
        "        \"message\": f\"Session {session_id} deleted successfully\"\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    \"\"\"Health check endpoint\"\"\"\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"active_sessions\": len(active_sessions),\n",
        "        \"default_keys_available\": app.state.agents is not None\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# Run the server if executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "id": "Tn_Hz8jKM8P2",
        "outputId": "7536e071-24ab-480f-b048-d6a810c58067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'lifespan' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e63680e7f400>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Initialize FastAPI app\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mapp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlifespan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlifespan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Add CORS middleware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lifespan' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the GraphState\n",
        "class GraphState(TypedDict):\n",
        "    messages: List[Any]  # The messages passed between user and assistant\n",
        "    conversation_history: Dict  # A record of all conversation threads\n",
        "    current_agent: str  # Track which agent is currently active\n",
        "    prompt_list: List[str]  # Store generated prompts\n",
        "    sample_images: List[str]  # Store URLs of sample images\n",
        "    user_feedback: str  # Store user feedback on sample images\n",
        "    dataset_ready: bool  # Flag to indicate if user wants the full dataset\n",
        "    llm_model: str  # Track which LLM model is being used\n",
        "    session_id: str  # Unique identifier for the conversation session\n",
        "    num_images: int  # Number of images for the dataset\n",
        "    image_resolution: str  # Resolution for generated images\n",
        "    groq_api_key: str  # API key for Groq\n",
        "    openai_api_key: str  # API key for OpenAI\n",
        "\n",
        "\n",
        "\n",
        "class AIResponse(BaseModel):\n",
        "    response: str\n",
        "    has_images: bool = False\n",
        "    image_urls: List[str] = []\n",
        "    session_id: str\n",
        "\n",
        "\n",
        "\n",
        "# Global variables to store active sessions\n",
        "active_sessions = {}\n",
        "\n",
        "from dotenv import load_dotenv()\n",
        "load_dotenv()\n",
        "\n",
        "# Default API keys - will be overridden by user-provided keys\n",
        "# default_groq_api_key = os.getenv(\"GROQ_API_KEY\", \"\")\n",
        "# default_openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "\n",
        "default_groq_api_key = userdata.get(\"groq_api_key\", \"\")\n",
        "default_openai_api_key = userdata.get(\"OPENAI_API_KEY\", \"\")\n",
        "\n",
        "\n",
        "if not default_groq_api_key or not default_openai_api_key:\n",
        "    print(\"Error: Missing API keys. Ensure GROQ_API_KEY and OPENAI_API_KEY are set in the environment.\")\n",
        "    print(f\"GROQ_API_KEY: {default_groq_api_key}\")\n",
        "    print(f\"OPENAI_API_KEY: {default_openai_api_key}\")\n",
        "\n",
        "\n",
        "# Agent initialization\n",
        "def initialize_agents(groq_api_key, openai_api_key):\n",
        "    \"\"\"Initialize all the agent components\"\"\"\n",
        "    try:\n",
        "        conversation_agent = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "        prompt_generator = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "        image_client = openai.OpenAI(api_key=openai_api_key)\n",
        "        router_agent = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "        return conversation_agent, prompt_generator, image_client, router_agent\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing agents: {e}\")\n",
        "\n",
        "        raise\n",
        "\n",
        "\n",
        "\n",
        "# Helper functions\n",
        "def extract_scenario(state):\n",
        "    \"\"\"Extract the user's scenario from the conversation history\"\"\"\n",
        "    relevant_messages = []\n",
        "\n",
        "    for message in state[\"messages\"]:\n",
        "        if isinstance(message, HumanMessage):\n",
        "            relevant_messages.append(message.content)\n",
        "\n",
        "    # Concatenate all user messages into a single scenario description\n",
        "    if relevant_messages:\n",
        "        combined_scenario = \" \".join(relevant_messages)\n",
        "        # If too long, use only the most substantial messages\n",
        "        if len(combined_scenario) > 500:\n",
        "            # Find the longest message, which likely contains the main scenario\n",
        "            longest_message = max(relevant_messages, key=len)\n",
        "            return longest_message\n",
        "        return combined_scenario\n",
        "\n",
        "    return \"generic dataset of various objects and scenes\"\n",
        "\n",
        "\n",
        "def get_current_user_question(state):\n",
        "    \"\"\"Extract the most recent user question from the state\"\"\"\n",
        "    for message in reversed(state[\"messages\"]):\n",
        "        if isinstance(message, HumanMessage):\n",
        "            return message.content\n",
        "    return \"\"\n",
        "\n",
        "# Generate a summary of conversation history\n",
        "def summarize_conversation(state):\n",
        "    \"\"\"Create a brief summary of the conversation history for context\"\"\"\n",
        "    history = []\n",
        "\n",
        "    # Extract last few turns of conversation\n",
        "    messages = state[\"messages\"][-10:]  # Limit to last 10 messages\n",
        "\n",
        "    for msg in messages:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            history.append(f\"User: {msg.content}\")\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            # Truncate very long assistant responses\n",
        "            content = msg.content\n",
        "            if len(content) > 200:\n",
        "                content = content[:197] + \"...\"\n",
        "            history.append(f\"Assistant: {content}\")\n",
        "\n",
        "    return \"\\n\".join(history)\n",
        "\n",
        "def parse_image_prompts(response_text):\n",
        "    \"\"\"Extract a list of prompts from the LLM response\"\"\"\n",
        "    try:\n",
        "        # Try to find a Python list in the text\n",
        "        match = re.search(r'\\[(.*?)\\]', response_text, re.DOTALL)\n",
        "        if match:\n",
        "            prompts_text = match.group(1)\n",
        "            # Safely evaluate the list\n",
        "            return eval(f\"[{prompts_text}]\")\n",
        "\n",
        "        # If no list is found, try to extract numbered items\n",
        "        lines = response_text.split('\\n')\n",
        "        prompts = []\n",
        "        for line in lines:\n",
        "            # Look for numbered lines like \"1. prompt\" or \"1) prompt\"\n",
        "            match = re.match(r'^\\s*\\d+[\\.\\)]\\s*(.*)', line)\n",
        "            if match:\n",
        "                prompts.append(match.group(1).strip())\n",
        "\n",
        "        if prompts:\n",
        "            return prompts\n",
        "\n",
        "        # If all else fails, split by newlines and filter\n",
        "        return [line.strip() for line in lines if line.strip() and len(line.strip()) > 10]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing prompts: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "# Defining the Router Properly\n",
        "class RouterOutput(BaseModel):\n",
        "    \"\"\"Output schema for the router agent\"\"\"\n",
        "    agent: Literal[\"conversation_agent\", \"sample_image_agent\", \"dataset_generator_agent\"]\n",
        "    confidence: float = Field(description=\"Confidence level from 0.0 to 1.0\")\n",
        "    reasoning: str = Field(description=\"Brief explanation of why this agent was chosen\")\n",
        "\n",
        "\n",
        "groq_api_key = state[\"groq_api_key\"] if state[\"groq_api_key\"] is not None else default_groq_api_key\n",
        "llm = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "router_agent = llm.with_structured_output(RouterOutput)\n",
        "\n",
        "system = \"\"\"You are an expert routing agent for an image dataset creation system.\n",
        "Your job is to analyze the user's message and determine which of the following agents should handle it:\n",
        "\n",
        "1. conversation_agent: For general conversation, questions about the system, or when the user is still describing their needs.\n",
        "\n",
        "2. sample_image_agent: When the user is explicitly or implicitly requesting sample images, wants to see examples,\n",
        "    or is describing specific image requirements that would benefit from visual confirmation before proceeding.\n",
        "\n",
        "3. dataset_generator_agent: When the user is ready to generate the full dataset of images, expressing satisfaction with samples,\n",
        "    or directly asking to proceed with dataset creation.\n",
        "\n",
        "Analyze the content, context, and intent of the user's message, then select the most appropriate agent.\n",
        "\n",
        "Output your decision as a JSON object with the following fields:\n",
        "- agent: The name of the agent (one of the three above)\n",
        "- confidence: Your confidence level in this decision (0.0 to 1.0)\n",
        "- reasoning: A brief explanation of why you chose this agent\n",
        "\"\"\"\n",
        "\n",
        "router_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system),\n",
        "    (\"human\", \"User message: {question}\\n\\nConversation history summary: {history_summary}\")\n",
        "])\n",
        "\n",
        "router_chain = router_prompt | router_agent\n",
        "\n",
        "\n",
        "\n",
        "# Routing function\n",
        "def route_question(state: GraphState, router_chain):\n",
        "    \"\"\"Route the user question to the appropriate agent\"\"\"\n",
        "\n",
        "    question = get_current_user_question(state)\n",
        "    history_summary = summarize_conversation(state)\n",
        "\n",
        "    # Make routing decision\n",
        "    try:\n",
        "        route_decision = router_chain.invoke({\n",
        "            \"question\": question,\n",
        "            \"history_summary\": history_summary\n",
        "        })\n",
        "\n",
        "        print(f\"Router Decision: {route_decision}\")\n",
        "\n",
        "        # Update state to track which agent was selected\n",
        "        state[\"current_agent\"] = route_decision.agent\n",
        "\n",
        "            # Direct routing based on router_chain output\n",
        "        if route_decision.agent == \"conversation_agent\":\n",
        "            print(\"---ROUTE QUESTION TO Conversation Agent---\")\n",
        "            return \"conversation\"  # Node name in LangGraph\n",
        "        elif route_decision.agent == \"sample_image_agent\":\n",
        "            print(\"---ROUTE QUESTION TO Sample Image Agent---\")\n",
        "            return \"generate_sample_images\"  # Node name in LangGraph\n",
        "        elif route_decision.agent == \"dataset_generator_agent\":\n",
        "            print(\"---ROUTE QUESTION TO Dataset Generator Agent---\")\n",
        "            state[\"dataset_ready\"] = True  # Set the flag if needed\n",
        "            return \"generate_prompts\"  # Node name in LangGraph\n",
        "\n",
        "        # Map agent names to graph nodes\n",
        "        # agent_to_node = {\n",
        "        #     \"conversation_agent\": \"conversation\",\n",
        "        #     \"sample_image_agent\": \"generate_sample_images\",\n",
        "        #     \"dataset_generator_agent\": \"generate_prompts\"\n",
        "        # }\n",
        "\n",
        "        # return agent_to_node[route_decision.agent]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Routing error: {e}\")\n",
        "        # Default to conversation as fallback\n",
        "        return \"conversation\"\n",
        "\n",
        "\n",
        "\n",
        "# Agent handlers\n",
        "def conversation_handler(state: GraphState):\n",
        "    \"\"\"The primary conversation agent to interact with the user\"\"\"\n",
        "\n",
        "    user_message = state[\"messages\"][-1].content\n",
        "\n",
        "    system_message = \"\"\"You are an AI assistant specializing in helping users create image datasets.\n",
        "    Your role is to understand what kind of image dataset the user wants to create.\n",
        "\n",
        "    Be conversational and helpful, focusing on understanding the user's requirements clearly.\n",
        "    If the user seems to be describing a specific type of images, suggest they might want to see samples.\n",
        "    If they've seen samples and like them, you can suggest generating the full dataset.\n",
        "\n",
        "    Keep your responses concise and focused on helping create their image dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_message),\n",
        "        (\"human\", \"{user_input}\")\n",
        "    ])\n",
        "\n",
        "    conversation_agent = state[\"conversation_agent\"]\n",
        "    chain = prompt | conversation_agent\n",
        "    response = chain.invoke({\"user_input\": user_message})\n",
        "\n",
        "    # Add to conversation history\n",
        "    conversation_id = len(state[\"conversation_history\"]) + 1\n",
        "    state[\"conversation_history\"][conversation_id] = {\n",
        "        \"question\": user_message,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "    state[\"messages\"].append(AIMessage(content=response.content))\n",
        "    state[\"current_agent\"] = \"conversation_agent\"\n",
        "\n",
        "    return \"router\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def sample_image_handler(state: GraphState):\n",
        "    \"\"\"Generate sample images for user review\"\"\"\n",
        "\n",
        "    # Extract user scenario from conversation\n",
        "    user_scenario = extract_scenario(state)\n",
        "\n",
        "    # Generate a few sample prompts\n",
        "    system = \"\"\"You are a prompt generator for text-to-image models.\n",
        "    Generate 3 diverse and detailed prompts for sample images based on the user scenario.\n",
        "    Each prompt should be descriptive and optimized for image generation.\n",
        "    Format your output as a Python list of exactly 3 strings: [\"prompt1\", \"prompt2\", \"prompt3\"]\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system),\n",
        "        (\"human\", f\"Generate 3 sample image prompts for scenario: {user_scenario}\")\n",
        "    ])\n",
        "\n",
        "    prompt_generator = state[\"prompt_generator\"]\n",
        "    chain = prompt | prompt_generator\n",
        "    response = chain.invoke({})\n",
        "\n",
        "    # Extract the prompt list\n",
        "    sample_prompts = parse_image_prompts(response.content)\n",
        "\n",
        "    # Ensure we have exactly 3 prompts\n",
        "    if len(sample_prompts) < 3:\n",
        "        # Fill in missing prompts\n",
        "        while len(sample_prompts) < 3:\n",
        "            sample_prompts.append(f\"{user_scenario} variation {len(sample_prompts)+1}\")\n",
        "    elif len(sample_prompts) > 3:\n",
        "        # Trim to just 3 prompts\n",
        "        sample_prompts = sample_prompts[:3]\n",
        "\n",
        "    # Generate sample images\n",
        "    sample_image_urls = []\n",
        "    for prompt in sample_prompts:\n",
        "        try:\n",
        "            response = state[\"image_client\"].images.generate(\n",
        "                model=\"dall-e-2\",\n",
        "                prompt=prompt,\n",
        "                size=state[\"image_resolution\"],  # Use the resolution from state\n",
        "                quality=\"standard\",\n",
        "                n=1\n",
        "            )\n",
        "            sample_image_urls.append(response.data[0].url)\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating image: {str(e)}\")\n",
        "            sample_image_urls.append(None)\n",
        "\n",
        "    # Filter out any None values\n",
        "    sample_image_urls = [url for url in sample_image_urls if url is not None]\n",
        "\n",
        "    state[\"sample_images\"] = sample_image_urls\n",
        "    state[\"prompt_list\"] = sample_prompts\n",
        "\n",
        "    # Create response message with sample images\n",
        "    response_text = \"Here are some sample images based on your requirements:\\n\\n\"\n",
        "    for i, (prompt, url) in enumerate(zip(sample_prompts, sample_image_urls)):\n",
        "        if url:\n",
        "            response_text += f\"Sample {i+1}:\\nPrompt: {prompt}\\n\\n\"\n",
        "\n",
        "    response_text += f\"How do these look? Would you like to make any adjustments before generating the full dataset of {state['num_images']} images?\"\n",
        "\n",
        "    state[\"messages\"].append(AIMessage(content=response_text))\n",
        "    state[\"current_agent\"] = \"sample_image_agent\"\n",
        "\n",
        "    return \"router\"\n",
        "\n",
        "def prompt_generator_handler(state: GraphState):\n",
        "    \"\"\"Generate the full set of prompts based on user requirements\"\"\"\n",
        "\n",
        "    # Extract user scenario from conversation\n",
        "    user_scenario = extract_scenario(state)\n",
        "    user_feedback = state.get(\"user_feedback\", \"\")\n",
        "\n",
        "    # Include user feedback if available\n",
        "    scenario_with_feedback = user_scenario\n",
        "    if user_feedback:\n",
        "        scenario_with_feedback = f\"{user_scenario}. User feedback on samples: {user_feedback}\"\n",
        "\n",
        "    # Use the custom number of images from state\n",
        "    num_images = state[\"num_images\"]\n",
        "\n",
        "    system = f\"\"\"You are a prompt generator for text to image models. Your task is to generate {num_images} diverse prompts\n",
        "    for text-to-image models to create a comprehensive dataset given the scenario.\n",
        "    Each prompt should be descriptive and optimized for image generation.\n",
        "    Make sure to cover different angles, perspectives, environments, and contexts.\n",
        "    Format your output as a Python list of strings: [\"prompt1\", \"prompt2\", ...]\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system),\n",
        "        (\"human\", f\"Generate {num_images} diverse image prompts for scenario: {scenario_with_feedback}\")\n",
        "    ])\n",
        "\n",
        "    prompt_generator = state[\"prompt_generator\"]\n",
        "    chain = prompt | prompt_generator\n",
        "    response = chain.invoke({})\n",
        "\n",
        "    # Extract the prompt list\n",
        "    prompt_list = parse_image_prompts(response.content)\n",
        "\n",
        "    # Ensure we have at least the required number of prompts\n",
        "    if len(prompt_list) < num_images:\n",
        "        # Fill in missing prompts\n",
        "        while len(prompt_list) < num_images:\n",
        "            prompt_list.append(f\"{user_scenario} variation {len(prompt_list)+1}\")\n",
        "\n",
        "    state[\"prompt_list\"] = prompt_list[:num_images]  # Limit to exactly the required number\n",
        "    state[\"dataset_ready\"] = True\n",
        "\n",
        "    # Prepare response message\n",
        "    response_text = f\"I've generated {num_images} diverse prompts for your dataset based on your requirements.\\n\\n\"\n",
        "    response_text += \"Here are a few examples:\\n\"\n",
        "    for i, prompt in enumerate(prompt_list[:5]):\n",
        "        response_text += f\"{i+1}. {prompt}\\n\"\n",
        "\n",
        "    response_text += f\"\\n...and {len(prompt_list)-5} more prompts.\\n\\n\"\n",
        "    response_text += f\"Would you like me to proceed with generating all {num_images} images for your dataset now?\"\n",
        "\n",
        "    state[\"messages\"].append(AIMessage(content=response_text))\n",
        "    state[\"current_agent\"] = \"dataset_generator_agent\"\n",
        "\n",
        "    return \"router\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def dataset_generator_handler(state: GraphState):\n",
        "    \"\"\"Generate the full dataset of images\"\"\"\n",
        "\n",
        "    try:\n",
        "        num_images = state[\"num_images\"]\n",
        "        prompt_list = state[\"prompt_list\"][:num_images]  # Ensure we use max the required number of prompts\n",
        "        image_urls = []\n",
        "\n",
        "        # Use custom resolution from state\n",
        "        resolution = state[\"image_resolution\"]\n",
        "\n",
        "        # Create a progress update message\n",
        "        progress_message = f\"Starting to generate your dataset with {num_images} images at {resolution} resolution. This might take some time...\\n\"\n",
        "        state[\"messages\"].append(AIMessage(content=progress_message))\n",
        "\n",
        "        # Generate images for each prompt\n",
        "        for idx, prompt in enumerate(prompt_list):\n",
        "            try:\n",
        "                print(f\"[{idx+1}/{len(prompt_list)}] Generating image for prompt: {prompt}\")\n",
        "\n",
        "                response = state[\"image_client\"].images.generate(\n",
        "                    model=\"dall-e-2\",\n",
        "                    prompt=prompt,\n",
        "                    size=resolution,\n",
        "                    quality=\"standard\",\n",
        "                    n=1\n",
        "                )\n",
        "\n",
        "                image_url = response.data[0].url\n",
        "                image_urls.append(image_url)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating image {idx+1}: {str(e)}\")\n",
        "                image_urls.append(None)\n",
        "\n",
        "        # Filter out None values\n",
        "        image_urls = [url for url in image_urls if url is not None]\n",
        "\n",
        "        # Final response with dataset information\n",
        "        final_message = f\"âœ… Dataset generation complete! Generated {len(image_urls)}/{num_images} images at {resolution} resolution.\\n\\n\"\n",
        "\n",
        "        # Include some sample URLs\n",
        "        if image_urls:\n",
        "            final_message += \"Here are a few sample images from your dataset:\\n\"\n",
        "            for i, url in enumerate(image_urls[:5]):\n",
        "                if url:\n",
        "                    final_message += f\"Image {i+1}: {url}\\n\"\n",
        "\n",
        "        final_message += \"\\nYou can download these images to assemble your complete dataset. Would you like me to help with anything else?\"\n",
        "\n",
        "        state[\"messages\"].append(AIMessage(content=final_message))\n",
        "        state[\"current_agent\"] = \"dataset_generator_agent\"\n",
        "\n",
        "        return \"router\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error in dataset_generator_handler: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "\n",
        "# Router handler function\n",
        "def router_handler(state: GraphState):\n",
        "    \"\"\"Route the conversation to the appropriate handler based on LLM decision\"\"\"\n",
        "    router_chain = state[\"router_chain\"]\n",
        "    return route_question(state, router_chain)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "conversation_agent, prompt_generator, image_client, router_agent = agents\n",
        "\n",
        "# Create the graph\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"conversation\", conversation_handler)\n",
        "workflow.add_node(\"generate_sample_images\", sample_image_handler)\n",
        "workflow.add_node(\"generate_prompts\", prompt_generator_handler)\n",
        "workflow.add_node(\"generate_dataset\", dataset_generator_handler)\n",
        "workflow.add_node(\"router\", router_handler)\n",
        "\n",
        "# Add edges from router to specific agents\n",
        "workflow.add_edge(\"router\", \"conversation\")\n",
        "workflow.add_edge(\"router\", \"generate_sample_images\")\n",
        "workflow.add_edge(\"router\", \"generate_prompts\")\n",
        "\n",
        "# Add edges from agents back to router\n",
        "workflow.add_edge(\"conversation\", \"router\")\n",
        "workflow.add_edge(\"generate_sample_images\", \"router\")\n",
        "workflow.add_edge(\"generate_prompts\", \"router\")\n",
        "\n",
        "# Add conditional edge from router to dataset generator\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\",\n",
        "    lambda state: state.get(\"dataset_ready\", False),\n",
        "    {\n",
        "        True: \"generate_dataset\",\n",
        "        False: \"router\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Dataset generator goes back to router when done\n",
        "workflow.add_edge(\"generate_dataset\", \"router\")\n",
        "\n",
        "# Compile the graph, designating the start point\n",
        "workflow.set_entry_point(\"router\") # THIS IS THE KEY ADDITION\n",
        "\n",
        "graph_runnable = workflow.compile()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Init GraphState\n",
        "def init_graph_state(session_id, agents, router_chain):\n",
        "    \"\"\"Create a new conversation session\"\"\"\n",
        "    conversation_agent, prompt_generator, image_client, router_agent = agents\n",
        "\n",
        "    # Initialize the state with default values\n",
        "    return GraphState(\n",
        "        messages=[AIMessage(content=\"Hello! I'm your image dataset creation assistant. What kind of image dataset would you like to create today?\")],\n",
        "        conversation_history={},\n",
        "        current_agent=\"conversation_agent\",\n",
        "        prompt_list=[],\n",
        "        sample_images=[],\n",
        "        user_feedback=\"\",\n",
        "        dataset_ready=False,\n",
        "        llm_model=\"Gemma2-9b-It\",\n",
        "        conversation_agent=conversation_agent,\n",
        "        prompt_generator=prompt_generator,\n",
        "        image_client=image_client,\n",
        "        router_agent=router_agent,\n",
        "        router_chain=router_chain,\n",
        "        session_id=session_id,\n",
        "        num_images=50,  # Default number of images\n",
        "        image_resolution=\"256x256\",  # Default resolution\n",
        "        groq_api_key=default_groq_api_key,\n",
        "        openai_api_key=default_openai_api_key\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to update agent components with new API keys\n",
        "def update_session_agents(session_id, groq_api_key, openai_api_key):\n",
        "    \"\"\"Update the agents for a session with new API keys\"\"\"\n",
        "    if session_id not in active_sessions:\n",
        "        return False\n",
        "\n",
        "    state = active_sessions[session_id]\n",
        "\n",
        "    # Update API keys in state\n",
        "    state[\"groq_api_key\"] = groq_api_key\n",
        "    state[\"openai_api_key\"] = openai_api_key\n",
        "\n",
        "    # Re-initialize agents with new keys\n",
        "    try:\n",
        "        conversation_agent = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "        prompt_generator = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "        image_client = openai.OpenAI(api_key=openai_api_key)\n",
        "        router_agent = ChatGroq(groq_api_key=groq_api_key, model=\"Gemma2-9b-It\")\n",
        "\n",
        "        # Update agents in state\n",
        "        state[\"conversation_agent\"] = conversation_agent\n",
        "        state[\"prompt_generator\"] = prompt_generator\n",
        "        state[\"image_client\"] = image_client\n",
        "        state[\"router_agent\"] = router_agent\n",
        "\n",
        "        # Also recreate the router chain\n",
        "        state[\"router_chain\"] = create_router(router_agent)\n",
        "\n",
        "        active_sessions[session_id] = state\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error updating session agents: {e}\")\n",
        "        return False\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mwGpV2fRoWie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HHNf60VjIro5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rGwBsZ5aCgzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tZclLE81Cgvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VoEetI2ZCgs9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}